* Duration
From 26 July 2020 to 
* Content
1. [[#day-31-dm][Day 31: Hierarchical Topic Mining via Joint Spherical Tree and Text Embedding]]
2. [[#day-32-se][Day 32: Simulee: Detecting CUDA Synchronization Bugs via Memory-Access Modeling]]
3. [[#day-33-se][Day 33: Identifying experts in software libraries and frameworks among GitHub users]]
4. [[#day-34-nlp][Day 34: DeeBERT: Dynamic Early Exiting for Accelerating BERT Inference]]
5. [[#day-35-ir][Day 35: Table Search Using a Deep Contextualized Language Model]]

* Day 31: DM
- *Title*: Hierarchical Topic Mining via Joint Spherical Tree and Text Embedding
- *Year*: 2020
- *Proc*: KDD

The following content is referred from [1]
** Keywords
Topic Mining; Topic Hierarchy; Text Embedding; Tree Embedding
** Problem
They propose a new task, *Hierarchical Topic Mining*, which takes only a topic hierarchy described by category names as user guidance, and aims to retrieve a set of coherent and representative terms under each category to help users comprehend his/her interested topics.

** Method
Hierarchical Topic Mining is weakly-supervised as it requires the user to provide the names of the hierarchy categories which serve as the minimal supervision and focuses on retrieving representative terms only for the provided categories.

** Result
The proposed model, named JoSH, mines a high-quality set of hierarchical topics with high efficiency and benefits weakly-supervised hierarchical text classification tasks.

** Future work
- One can extend JoSH to not only focus on a user-given category structure, but also be able to discover other latent topics from a text corpus, probably by relaxing the assumption that a document is generated from one ofthe given topics or collaborating with other taxonomy construction algorithms.

- Embedding tree or graph structures along with textual data in the spherical space for mining structured knowledge from text corpora.

* Day 32: SE
- *Title*: Simulee: Detecting CUDA Synchronization Bugs via Memory-Access Modeling
- *Year*: 2020
- *Proc*: ICSE

The following content is referred from [2]
** Problem
How to effectively and efficiently detect CUDA synchronization bugs remains a challenging open problem.

** Method
They pro-pose the first lightweight CUDA synchronization bug detection framework, namely Simulee, to model CUDA program execution by interpreting the corresponding LLVM bytecode and collecting the memory-access information for automatically detecting general CUDA synchronization bugs.

** Result
Simulee can detect 21 out of the 24 manually identified bugs in our preliminary study and also 24 previously unknown bugs among all projects, 10 of which have already been confirmed by the developers.

The results suggest that Simulee is able to detect most of the manually identified synchronization bugs in the benchmark.

* Day 33: SE
- *Title*: Identifying experts in software libraries and frameworks among GitHub users

- *Year*: 2019
- *Proc*: MSR

The following content is referred from [3]
** Problem
We still lack techniques to assess developers expertise in widely popular libraries and frameworks.

** Method
They evaluate the performance of unsupervised (based on clustering) and supervised machine learning classifiers (Random Forest and SVM) to identify experts in three popular JavaScript libraries: facebook/react, mongodb/node-mongodb, and socketio/socket.io.

** Result
First, they found that standard machine learning classifiers (e.g., Random Forest and SVM) do not have a good performance in this problem, at least when they are trained with all developers from a sample of GitHub users. The main reason is that not all experts have a strong presence on GitHub. By contrast, they used clustering techniques to identify experts with high activity on GitHub projects that depend on particular libraries and frameworks. Particularly, they found clusters with 74% (REACT), 65% (NODE-MONGODB), and 75% (SOCKET.IO) of experts.

** Future work
(1) investigate other target libraries and frameworks; 
(2) investigate the use of features from other platforms, such as Stack Overflow and TopCoder;
(3) investigate the accuracy of the proposed method with other developers, including developers of less popular projects

* Day 34: NLP
- *Title*: DeeBERT: Dynamic Early Exiting for Accelerating BERT Inference
- *Year*: 2020
- *Proc*: NLP

The following content is referred from [4]

** Problem
Large-scale pre-trained language models are slow in inference.

** Method
They propose DeeBERT (Dynamic early exiting for BERT) to accelerate BERT.

The inspiration comes from a well-known observation in the computer vision community: in deep convolutional neural networks, higher layers typically produce more detailed and finer-grained features.

DeeBERT accelerates BERT inference by inserting extra classification layers (which we refer to as off-ramps) between each transformer layer of BERT.

There is no early stopping and the checkpoint after full fine-tuning is chosen.

** Result
They conduct experiments on BERT and RoBERTa with six GLUE datasets, showing that DeeBERT is capable of accelerating model inference by up to ∼40% with minimal model quality degradation on downstream tasks.

DeeBERT, an effective method that exploits redundancy in BERT models to achieve better quality–efficiency trade-offs.

** Future work
(1) DeeBERT’s training method, while maintaining good quality in the last off-ramp, reduces model capacity available for intermediate off-ramps; it would be important to look for a method that achieves a better balance between all off-ramps.

(2) The reasons why some transformer layers appear redundant2 and why DeeBERT considers some samples easier than others remain unknown; it would be interesting to further explore relationships between pre-training and layer redundancy, sample complexity and exit layer, and related characteristics.

* Day 35: IR
- *Title*: Table Search Using a Deep Contextualized Language Model
- *Year*: 2020
- *Proc*: SIGIR

The following content is referred from [5]
** Problem
They consider the task ofad hoc table retrieval where given a keyword query, a list of ranked tables are returned.

They use the deep contextualized language model BERT for the task of ad hoc table retrieval. They investigate how to encode table content considering the table structure and input length limit of BERT. We also propose an approach that incorporates features from prior literature on table retrieval and jointly trains them with BERT.

** Method
In experiments on public datasets, they show that their best approach can outperform the previous state-of-the-art method and BERT baselines with a large margin under different evaluation metrics.

** Result
Our proposed Hybrid-BERT-Row-Max method outperforms the previous state-of-the-art and BERT baselines with a large margin on WikiTables dataset.

** Future work
Future work could design a framework that automatically chooses the strategy considering the query types. Besides, designing pretraining tasks for tables and pretraining BERT on a large table collection could be promising to further improve the performance of BERT on table-related tasks such as table retrieval.

* Reference
1. Meng, Y., Zhang, Y., Huang, J., Zhang, Y., Zhang, C., & Han, J. (2020). Hierarchical Topic Mining via Joint Spherical Tree and Text Embedding. arXiv preprint arXiv:2007.09536.

2. APA is unavailable now

3. Montandon, J. E., Silva, L. L., & Valente, M. T. (2019, May). Identifying experts in software libraries and frameworks among GitHub users. In 2019 IEEE/ACM 16th International Conference on Mining Software Repositories (MSR) (pp. 276-287). IEEE.

4. Xin, J., Tang, R., Lee, J., Yu, Y., & Lin, J. (2020). DeeBERT: Dynamic Early Exiting for Accelerating BERT Inference. arXiv preprint arXiv:2004.12993.

5. Chen, Z., Trabelsi, M., Heflin, J., Xu, Y., & Davison, B. D. (2020). Table Search Using a Deep Contextualized Language Model. arXiv preprint arXiv:2005.09207.