* Duration
From 26 July 2020 to 24 August 2020

* Content
1. [[#day-31-dm][Day 31: Hierarchical Topic Mining via Joint Spherical Tree and Text Embedding]]
2. [[#day-32-se][Day 32: Simulee: Detecting CUDA Synchronization Bugs via Memory-Access Modeling]]
3. [[#day-33-se][Day 33: Identifying experts in software libraries and frameworks among GitHub users]]
4. [[#day-34-nlp][Day 34: DeeBERT: Dynamic Early Exiting for Accelerating BERT Inference]]
5. [[#day-35-ir][Day 35: Table Search Using a Deep Contextualized Language Model]]
6. [[#day-36-ir][Day 36: An Analysis of BERT in Document Ranking]]
7. [[#day-37-ir][Day 37: A Pairwise Probe for Understanding BERT Fine-Tuning on Machine Reading Comprehension]]
8. [[#day-38-se][Day 38: TRADER: Trace Divergence Analysis and Embedding Regulation for Debugging Recurrent Neural Networks]]
9. [[#day-39-nlp][Day 39: Beyond Accuracy: Behavioral Testing of NLP Models with CheckList]]
10. [[#day-40-se][Day 40: Here We Go Again: Why Is It Difficult for Developers to Learn Another Programming Language?]]
11. [[#day-41-se][Day 41: Deep Transfer Bug Localization]]
12. [[#day-42-se][Day 42: Causal Testing: Understanding Defects’ Root Causes]]
13. [[#day-43-se][Day 43: An Empirical Study on API Parameter Rules Hao]]
14. [[#day-44-se][Day 44: Unblind Your Apps: Predicting Natural-Language Labels for Mobile GUI Components by Deep Learning]]
15. [[#day-45-se][Day 45: Wireframe-based UI Design Search through Image Autoencoder]]
16. [[#day-46-se][Day 46: Repairing Deep Neural Networks: Fix Patterns and Challenges]]
17. [[#day-47-se][Day 47: Fuzz Testing based Data Augmentation to Improve Robustness of Deep Neural Networks]]
18. [[#day-48-se][Day 48: Understanding the Automated Parameter Optimization on Transfer Learning for CPDP: An Empirical Study]]
19. [[#day-49-se][Day 49: CPC: Automatically Classifying and Propagating Natural Language Comments via Program Analysis]]
20. [[#day-50-se][Day 50: Gang of Eight: A Defect Taxonomy for Infrastructure as Code Scripts]]
21. [[#day-51-se][Day 51: Predictive Models in Software Engineering: Challenges and Opportunities]]
22. [[#day-52-se][Day 52: Where should I comment my code? A dataset and model for predicting locations that need comments]]
23. [[#day-53-se][Day 53: Better Code, Better Sharing: On the Need of Analyzing Jupyter Notebooks]]
24. [[#day-54-se][Day 54: Assessing Practitioner Beliefs about Software Defect Prediction]]
25. [[#day-55-se][Day 55: Skyline: Interactive In-Editor Computational Performance Profiling for Deep Neural Network Training]]
26. [[#day-56-se][Day 56: Suggesting Natural Method Names to Check Name Consistencies]]
27. [[#day-57-se][Day 57: Is Your Quantum Program Bug-Free?]]
28. [[#day-58-se][Day 58: Efficient Generation of Error-Inducing Floating-Point Inputs via Symbolic Execution]]
29. [[#day-59-se][Day 59: A Study on the Prevalence of Human Values in Software Engineering Publications, 2015 – 2018]]
30. [[#day-60-se][Day 60: Pre-trained Models for Natural Language Processing: A Survey]]


* Day 31: DM
- *Title*: Hierarchical Topic Mining via Joint Spherical Tree and Text Embedding
- *Year*: 2020
- *Proc*: KDD

The following content is referred from [1]
** Keywords
Topic Mining; Topic Hierarchy; Text Embedding; Tree Embedding
** Problem
They propose a new task, *Hierarchical Topic Mining*, which takes only a topic hierarchy described by category names as user guidance, and aims to retrieve a set of coherent and representative terms under each category to help users comprehend his/her interested topics.

** Method
Hierarchical Topic Mining is weakly-supervised as it requires the user to provide the names of the hierarchy categories which serve as the minimal supervision and focuses on retrieving representative terms only for the provided categories.

** Result
The proposed model, named JoSH, mines a high-quality set of hierarchical topics with high efficiency and benefits weakly-supervised hierarchical text classification tasks.

** Future work
- One can extend JoSH to not only focus on a user-given category structure, but also be able to discover other latent topics from a text corpus, probably by relaxing the assumption that a document is generated from one ofthe given topics or collaborating with other taxonomy construction algorithms.

- Embedding tree or graph structures along with textual data in the spherical space for mining structured knowledge from text corpora.

* Day 32: SE
- *Title*: Simulee: Detecting CUDA Synchronization Bugs via Memory-Access Modeling
- *Year*: 2020
- *Proc*: ICSE

The following content is referred from [2]
** Problem
How to effectively and efficiently detect CUDA synchronization bugs remains a challenging open problem.

** Method
They pro-pose the first lightweight CUDA synchronization bug detection framework, namely Simulee, to model CUDA program execution by interpreting the corresponding LLVM bytecode and collecting the memory-access information for automatically detecting general CUDA synchronization bugs.

** Result
Simulee can detect 21 out of the 24 manually identified bugs in our preliminary study and also 24 previously unknown bugs among all projects, 10 of which have already been confirmed by the developers.

The results suggest that Simulee is able to detect most of the manually identified synchronization bugs in the benchmark.

* Day 33: SE
- *Title*: Identifying experts in software libraries and frameworks among GitHub users

- *Year*: 2019
- *Proc*: MSR

The following content is referred from [3]
** Problem
We still lack techniques to assess developers expertise in widely popular libraries and frameworks.

** Method
They evaluate the performance of unsupervised (based on clustering) and supervised machine learning classifiers (Random Forest and SVM) to identify experts in three popular JavaScript libraries: facebook/react, mongodb/node-mongodb, and socketio/socket.io.

** Result
First, they found that standard machine learning classifiers (e.g., Random Forest and SVM) do not have a good performance in this problem, at least when they are trained with all developers from a sample of GitHub users. The main reason is that not all experts have a strong presence on GitHub. By contrast, they used clustering techniques to identify experts with high activity on GitHub projects that depend on particular libraries and frameworks. Particularly, they found clusters with 74% (REACT), 65% (NODE-MONGODB), and 75% (SOCKET.IO) of experts.

** Future work
(1) investigate other target libraries and frameworks; 
(2) investigate the use of features from other platforms, such as Stack Overflow and TopCoder;
(3) investigate the accuracy of the proposed method with other developers, including developers of less popular projects

* Day 34: NLP
- *Title*: DeeBERT: Dynamic Early Exiting for Accelerating BERT Inference
- *Year*: 2020
- *Proc*: NLP

The following content is referred from [4]

** Problem
Large-scale pre-trained language models are slow in inference.

** Method
They propose DeeBERT (Dynamic early exiting for BERT) to accelerate BERT.

The inspiration comes from a well-known observation in the computer vision community: in deep convolutional neural networks, higher layers typically produce more detailed and finer-grained features.

DeeBERT accelerates BERT inference by inserting extra classification layers (which we refer to as off-ramps) between each transformer layer of BERT.

There is no early stopping and the checkpoint after full fine-tuning is chosen.

** Result
They conduct experiments on BERT and RoBERTa with six GLUE datasets, showing that DeeBERT is capable of accelerating model inference by up to ∼40% with minimal model quality degradation on downstream tasks.

DeeBERT, an effective method that exploits redundancy in BERT models to achieve better quality–efficiency trade-offs.

** Future work
(1) DeeBERT’s training method, while maintaining good quality in the last off-ramp, reduces model capacity available for intermediate off-ramps; it would be important to look for a method that achieves a better balance between all off-ramps.

(2) The reasons why some transformer layers appear redundant2 and why DeeBERT considers some samples easier than others remain unknown; it would be interesting to further explore relationships between pre-training and layer redundancy, sample complexity and exit layer, and related characteristics.

* Day 35: IR
- *Title*: Table Search Using a Deep Contextualized Language Model
- *Year*: 2020
- *Proc*: SIGIR

The following content is referred from [5]
** Problem
They consider the task ofad hoc table retrieval where given a keyword query, a list of ranked tables are returned.

They use the deep contextualized language model BERT for the task of ad hoc table retrieval. They investigate how to encode table content considering the table structure and input length limit of BERT. We also propose an approach that incorporates features from prior literature on table retrieval and jointly trains them with BERT.

** Method
In experiments on public datasets, they show that their best approach can outperform the previous state-of-the-art method and BERT baselines with a large margin under different evaluation metrics.

** Result
Our proposed Hybrid-BERT-Row-Max method outperforms the previous state-of-the-art and BERT baselines with a large margin on WikiTables dataset.

** Future work
Future work could design a framework that automatically chooses the strategy considering the query types. Besides, designing pretraining tasks for tables and pretraining BERT on a large table collection could be promising to further improve the performance of BERT on table-related tasks such as table retrieval.

* Day 36: IR
- *Title*: An Analysis of BERT in Document Ranking
- *Year*: 2020
- *Proc*: SIGIR

The following content is referred from [6]
** Problem
To increase the explainability of the ranking process performed by BERT, we investigate a state-of-the-art BERT-based ranking model with focus on its attention mechanism and interaction behavior.

They believe this baseline is too simple, so whether and how BERT can learn good representations for queries and documents is not thoroughly investigated.

** Method
First, an attribution technique is used to study the token importance in different layers. 

Second, several probing classifiers are trained to study the relevance signal carried by the token representations. 

Third, they compare the performance of BERT when its attention matrix is masked in different ways to investigate the importance of interactions.

** Result
It demonstrates that BERT extracts query-independent representations for document. Thus, the representations ofdocument tokens can be pre-calculated offline to improve efficiency.

** Future work
Transforming BERT to a more efficient representation-focused model

* Day 37: IR
- *Title*: A Pairwise Probe for Understanding BERT Fine-Tuning on Machine Reading Comprehension
- *Year*: 2020
- *Proc*: SIGIR

The following content is referred from [7]
** Problem
In this paper, inspired by the observation that most probing tasks involve identifying matched pairs of phrases (e.g. coreference requires matching an entity and a pronoun), they propose a pairwise probe to understand BERT fine-tuning on the machine reading comprehension (MRC) task.

** Method
In order to probe the above phenomena, we design a pairwise ranking metric to quantitatively compare pre-trained and fine-tuned model with in-domain data. The metric is designed to measure whether matching pairs are closer than random un-matching pairs that aim to provide insight about how well related information are encoded.

** Result
(1) Fine-tuning has little effect on the fundamental and low-level information and general semantic tasks. 
(2) For specific abilities required for downstream tasks, fine-tuned BERT is better than pre-trained BERT and such gaps are obvious after the fifth layer

** Future work
One can apply the pairwise ranking metric to analyze impact of fine-tuning on other tasks.

* Day 38: SE
- *Title*: TRADER: Trace Divergence Analysis and Embedding Regulation for Debugging Recurrent Neural Networks
- *Year*: 2020
- *Proc*: ICSE

The following content is referred from [8]
** Problem
They propose a new technique to automatically diagnose how problematic embeddings impact model performance, by comparing model execution traces from correctly and incorrectly executed samples.

** Method
They focus on debugging RNN models for textual inputs (e.g., sentiment analysis for developer comments), especially for a type of bugs in which problematic word embeddings lead to suboptimal model accuracy.

** Result
The experiments show that TRADER can consistently and effectively improve accuracy for real world models and datasets by 5.37% on average, which represents substantial improvement in the literature of RNN models.

* Day 39: NLP
- *Title*: Beyond Accuracy: Behavioral Testing of NLP Models with CheckList
- *Year*: 2020
- *Proc*: ACL

The following content is referred from [9]
** Problem
While useful, accuracy on benchmarks is not sufficient for evaluating NLP models.

** Method
They introduce CheckList, a task-agnostic methodology for testing NLP models

CheckList includes a matrix of general linguistic capabilities and test types that facilitate comprehensive test ideation, as well as a software tool to generate a large and diverse number of test cases quickly. 

** Result
They illustrate the utility of CheckList with tests for three tasks, identifying critical failures in both commercial and state-of-art models. In a user study, a team responsible for a commercial sentiment analysis model found new and actionable bugs in an extensively tested model. In another user study, NLP practitioners with CheckList created twice as many tests, and found almost three times as many bugs as users without it.

* Day 40: SE
- *Title*: Here We Go Again: Why Is It Difficult for Developers to Learn Another Programming Language?
- *Year*: 2020
- *Proc*: SE

The following content is referred from [10]
** Problem
To understand if programmers have difficulty learning additional programming languages, they conducted an empirical study of Stack Overflow questions across 18 different programming languages.

** Method
They hypothesized that previous knowledge could potentially interfere with learning a new programming language. From their inspection of 450 Stack Overflow questions, they found 276 instances of interference that occurred due to faulty assumptions originating from knowledge about a different language.

They analyzed 450 posts for 18 different programming languages and qualitatively coded each post, characterizing posts in terms of whether or not programmers made incorrect assumptions based on their previous programming knowledge. Then, to understand what learning strategies programmers used when learning another language and why previous knowledge could interfere with this process. They interviewed 16 professional programmers who had recently switched to a new programming language.

** Result
- Cross-language interference is a problem: 276 (61%) cross-language posts on Stack Overflow contained incorrect assumptions due to interference with previous language knowledge.

- Based on our interviews, professional programmers primarily learned new languages on their own, using an opportunistic strategy that often involved relating the new language to previous language knowledge; however, this results in interference which harms their learning.

- Learning a new language involves breaking down old habits, shifting one’s mindset, dealing with little-to-no mapping to previous languages, searching for proper documentation, and retooling in a new environment. All together, these challenges make learning another language difficult.

* Day 41: SE
- *Title*: Deep Transfer Bug Localization Xuan
- *Year*: 2019
- *Journal*: TSE

The following content is referred from [11]
** Problem
Sufficient bug data is often unavailable for many projects and companies. This raises the need for cross-project bug localization – the use of data from a project to help locate bugs in another project

** Method
They propose a deep transfer learning approach for cross-project bug localization. The proposed approach named TRANP-CNN extracts transferable semantic features from source project and fully exploits labeled data from target project for effective cross-project bug localization.

They proposed a novel deep transfer neural network named TRANP-CNN (TRAnsfer Natural and Program Language Convolutional Neural Network). Firstly, TRANP-CNN takes bug reports and source files as inputs and learns a common transferable latent feature representation shared by both source and target projects. Next, TRANP-CNN creates a pair of prediction functions that are biased towards the source and target projects, based on the shared feature representation.

TRANP-CNN consists of four layers: input layer, transferable feature extraction layer, project-specific prediction layer and output layer.

** Result
TRANP-CNN can locate buggy files correctly at top 1, top 5, and top 10 positions for 29.9%, 51.7%, 61.3% of the bugs respectively, which significantly outperform state-of-the-art bug localization solution based on deep learning and several other advanced alternative solutions considering various standard evaluation metrics.

** Future work
They plan to extend the evaluation of TRANP-CNN by including more bug reports from additional projects. They also plan to develop our solution into a tool that is integrated with an IDE followed by its evaluation from industry partners.

* Day 42: SE
- *Title*: Causal Testing: Understanding Defects’ Root Causes
- *Year*: 2020
- *Proc*: ICSE

The following content is referred from [12]
** Problem
Debugging and understanding software behavior is an important part of building software systems. To address this shortcoming of modern debugging tools, this
paper presents CausalTesting, a novel technique for identifying root causes of failing executions based on the theory of counterfactual causality.

** Method
To address this shortcoming of modern debugging tools, this paper presents CausalTesting, a novel technique for identifying root causes of failing executions based on the theory of counterfactual causality.

** Result
Using the Defects4J benchmark, we find that Causal Testing could be applied to 71% of real-world defects, and for 77% of those, it can help developers identify the root cause of the defect.

** Future work
Future work could extend Causal Testing to include oracle mutation. A fruitful line of research, when specifications, formal or informal, are available, is to extract oracles from those specifications.

* Day 43: SE
- *Title*: An Empirical Study on API Parameter Rules Hao
- *Year*: 2020
- *Proc*: ICSE

The following content is referred from [13]
** Problem
API libraries have been widely used, but are often poorly documented. When programmers do not fully understand API usage, they can introduce API-related bugs into their code. To handle this issue, researchers have proposed various approaches to facilitate better API usage. In particular, a popular research area is to mine parameter rules for APIs. To help developers correctly use library APIs, researchers built tools to mine API parameter rules. However, it is still unknown (1) what types of parameter rules there are, and (2) how these rules distribute inside documents and source files.

** Method
They conducted an empirical study to investigate the above-mentioned questions. To analyze as many parameter rules as possible, they took a hybrid approach that combines automatic localization of constrained parameters with manual inspection.

The automatic approach—PaRu—locates parameters that have constraints either documented in Javadoc (i.e., document rules) or implied by source code (i.e., code rules). Our manual inspection (1) identifies and categorizes rules for the located parameters, and (2) establishes mapping between document and code rules. By applying PaRu to 9 widely used libraries, we located 5,334 parameters with either document or code rules. Interestingly, there are only 187 parameters that have both types of rules, and 79 pairs of these parameter rules are unmatched. Additionally, PaRu extracted 1,688 rule sentences from Javadoc and code. We manually classified these sentences into six categories, two of which are overlooked by prior approaches.

** Result
We found that 86.2% of parameters have only code rules; 10.3% of parameters have only document rules; and only 3.5% of parameters have both document and code rules.

** Future work
work towards better mining and recommendation techniques for parameter rules

* Day 44: SE
- *Title*: Unblind Your Apps: Predicting Natural-Language Labels for Mobile GUI Components by Deep Learning
- *Year*: 2020
- *Prco*: ICSE

The following content is referred from [14]
** Problem
The prerequisite of using screen readers is that developers have to add natural-language labels to the image-based components when they are developing the app. Unfortunately, more than 77% apps have issues of missing labels, according to their analysis of 10,408 Android apps.

** Method
To overcome those challenges, they develop a deep learning based
model to automatically predict the content description.

Inspired by image captioning, they adopt the CNN and transformer
encoder decoder for predicting the labels based on the large-scale dataset.
** Result
The experiments show that our LabelDroid can achieve 60.7% exact match and 0.654 ROUGE-L score which outperforms both state-of-the-art baselines. We also demonstrate that the predictions from our model is of higher quality than that from junior Android developers.
** Future work
In the future, they will first improve our model for achieving better quality by taking the app metadata into the consideration. Second, they will also try to test the quality of existing labels by checking if the description is concise and informative.

* Day 45: SE
- *Title*: Wireframe-based UI Design Search through Image Autoencoder
- *Year*: 2020
- *Journal*: TOSEM

The following content is referred from [15]
** Problem
Existing keyword-based, image-similarity-based, and component-matching-based methods cannot reliably find relevant high-fidelity UI designs in a large database alike to the UI wireframe that the developers sketch, in face ofthe great variations in UI designs.

** Method
The key innovation of their search engine is to train a wireframe image autoencoder using a large database of real-application UI designs, without the need for labeling relevant UI designs.

** Result
Our experiments confirm the superior performance of our search engine over existing image-similarity or component-matching-based methods and demonstrate the usefulness of their search engine in real-world UI design tasks.

** Future work
One can extend of the tool to collecting UI elements in WebView components and in specific engine.

* Day 46: SE
- *Title*: Repairing Deep Neural Networks: Fix Patterns and Challenges
- *Year*: 2020
- *Proc*: ICSE

The following content is referred from [16]
** Problem
A significant SE problem in the software that uses DNNs is the
presence of bugs. What are the common bugs in such software? How do they differ? Answering these questions has the potential to fuel SE research on bug detection and repair for DNNs. This work focuses on bug fix patterns.

** Method
They have studied 415 repairs from Stack Overflow and 555 repairs from GitHub for five popular deep learning libraries Caffe, Keras, Tensorflow, Theano, and Torch to understand challenges in repairs and bug repair patterns.

** Result
Their key findings reveal that DNN bug fix patterns are distinctive compared to traditional bug fix patterns; the most common bug fix patterns are fixing data dimension and neural network connectivity; DNN bug fixes have the potential to introduce adversarial vulnerabilities; DNN bug fixes frequently introduce new bugs; and DNN bug localization, reuse of trained model, and coping with frequent releases are major challenges faced by developers when fixing bugs. We also contribute a benchmark of 667 DNN (bug, repair) instances.

** Future work
First and perhaps most immediately, a number of bug fix patterns identified by this work can be automated in repair tools. Such tools for bug repairs can help the developers integrating DNN into their software. Second, an abstract representation of the DNN along with the code that uses it can be developed. We saw several bug fix patterns that rely on analyzing such a representation. Third, there is a critical need to improve bug localization for DNN by addressing unique challenges that arise, and by creating DNN-aware bug localization tools. Fourth, there is an urgent need to detect bugs introduced by dimension mismatch and specially changes that have the potential to introduce vulnerabilities in the DNNs. Fifth, urgent work is needed on upgrade tools that encode the semantics of version changes and keep up with the change in the signature and semantics of DNN libraries.

* Day 47: SE
- *Title*: Fuzz Testing based Data Augmentation to Improve Robustness of Deep Neural Networks
- *Year*: 2020
- *Proc*: ICSE

The following content is referred from [17]
** Problem
Deep neural networks (DNN) have been shown to be notoriously brittle to small perturbations in their input data. This problem is analogous to the over-fitting problem in test-based program synthesis and automatic program repair, which is a consequence of the incomplete specification, i.e., the limited tests or training examples, that the program synthesis or repair algorithm has to learn from.

** Method
They propose a technique that re-purposes software testing methods, specifically mutation-based fuzzing, to augment the training data of DNNs, with the objective of enhancing their robustness. Our technique casts the DNN data augmentation problem as an optimization problem. It uses genetic search to generate the most suitable variant of an input data to use for training the DNN.

They propose a new algorithm that uses guided test generation techniques to address the data aug- mentation problem for robust generalization of DNNs under natural environmental variations. Specifically, we cast data augmentation problem as an optimization problem, and use genetic search on a space of the natural environmental variants of each training input data, to identify the worst variant for augmentation.
** Result
Our evaluation shows that Sensei can improve the robust accuracy of the DNN, compared to the state of the art, on each of the 15 models, by upto 11.9% and 5.5% on average. Further, Sensei-SA can reduce the average DNN training time by 25%, while still improving robust accuracy.

** Future work
Consider combination of two approaches, theirs and others.

* Day 48: SE
- *Title*: Understanding the Automated Parameter Optimization on Transfer Learning for CPDP: An Empirical Study
- *Year*: 2020
- *Proc*: ICSE

The following content is referred from [18]
** Problem
Most CPDP techniques involve two major steps, i.e., transfer learning and classification, each of which has at least one parameter to be tuned to achieve their optimal performance. This practice fits well with the purpose of automated parameter optimization. However, there is a lack of thorough understanding about what are the impacts of automated parameter optimization on various CPDP techniques.

** Method
They present the first empirical study that looks into such impacts on 62 CPDP techniques, 13 of which are chosen from the existing CPDP literature while the other 49 ones have not been explored before.

** Result
(1) Automated parameter optimization substantially improves the defect prediction performance of 77% CPDP techniques with a manageable computational cost.
(2) Transfer learning is of ultimate importance in CPDP.
(3) The research on CPDP is far from mature where it is 'not difficult' to find a better alternative by making a combination of existing transfer learning and classification techniques.

** Future work
One can design sophisticated optimizer for CPDP that explicitly searches the parameter space for the transfer learning part. Furthermore, the problem of portfolio optimization for CPDP, which involves both the selection of combination and parameter tuning, is also one of our ongoing research directions.

Future work should target a whole portfolio of optimization, tuning not only the parameters, but also the algorithmic components, i.e., the selection of appropriate transfer learning and classifier pair, of a CPDP model.

* Day 49: SE
- *Title*: CPC: Automatically Classifying and Propagating Natural Language Comments via Program Analysis
- *Year*: 2020
- *Proc*: ICSE

The following content is referred from [19]
** Problem
Developers are less motivated to write and update comments, making it infeasible and error-prone to leverage comments to facilitate software engineering task

** Method
They propose to leverage program analysis to systematically derive, refine, and propagate comments

** Result
They evaluate it on 5 large real-world projects. The evaluation results demonstrate that 41573 new comments can be derived by propagation from other code locations with 88% accuracy.

** Future work
They will train the classiiers with more labeled comments of other kinds of systems to improve the generalizability.

* Day 50: SE
- *Title*: Gang of Eight: A Defect Taxonomy for Infrastructure as Code Scripts
- *Year*: 2020
- *Proc*: ICSE

The following content is referred from [20]
** Problem
The goal ofthis paper is to help practitioners improve the quality of infrastructure as code (IaC) scripts by developing a defect taxonomy for IaC scripts through qualitative analysis.

** Method
We develop a taxonomy of IaC defects by applying qualitative analysis on 1,448 defect-related commits collected from open source software (OSS) repositories of the Openstack organization. We conduct a survey with 66 practitioners to assess if they agree with the identified defect categories included in our taxonomy. We quantify the frequency of identified defect categories by analyzing 80,425 commits collected from 291 OSS repositories spanning across 2005 to 2019.

** Result
Using our reported defect category frequency results, practitioners can prioritize V&V efforts by fixing configuration data defects that occur in 23.5%∼33.9% of IaC scripts.

** Future work
Researchers can investigate if above-mentioned recommendations can actually reduce defects in IaC scripts. The coding patterns that ACID use, could be further leveraged in investigating if defect categories for IaC, such as configuration data, can be detected at compile time.

* Day 51: SE
- *Title*: Predictive Models in Software Engineering: Challenges and Opportunities
- *Year*: 2020

The following content is referred from [21]
** Problem
They describe the key models and approaches used, classify the different models, summarize the range of key application areas, and analyze research results. Based on their findings, they also propose a set of current challenges that still need to be addressed in future work and provide a proposed research road map for these opportunities.

** Method
Following previous survey study approaches, they first collected the titles of all papers published at ICSE, ASE, FSE, TSE, TOSEM, and EMSE between 2009 and 2019 from DBLP computer science bibliography.
** Result
- The cumulative number of predictive model related studies shows an increasing trend over the last decade, and most of the selected primary studies focus on proposing novel approaches.
- We found 52 different predictive models were employed in software engineering tasks. These models can be classified into three categories – base learners, ensemble learners and deep learners.
- Logistic Regression and Naive Bayes are the most widely used learning techniques to build predictive models for SE tasks to date. Several machine learning models are also popular models for addressing specific problems, including SVM and decision trees.
- Recall, precision, and F-measure are the most frequently used performance metrics for evaluating the effectiveness of predictive models.
** Future work
1. Leveraging the power of big data.
2. Neural network based predictive models.
3. Assessment and selection of predictive models
4. predictive models in specific research domains

* Day 52: SE
- *Title*: Where should I comment my code? A dataset and model for predicting locations that need comments
- *Year*: 2020
- *Proc*: ICSE-NIER

The following content is referred from [22]
** Problem
They have created a machine learning model that suggests locations where a programmer should write a code comment.
** Method
They present a corpus of C code where comment locations are identified.

Their goal is to produce a binary label for each snippet indicating whether it should be commented or not. The comment might be needed on the snippet as a whole or on some LOC within it. They evaluated several models for this task

To create their dataset, they divided the source code into snippets, labeled each snippet depending on whether it contains a comment, and removed all comments from the snippets.

** Result
Their models achieved precision of 74% and recall of 13% in identifying comment-worthy locations.

** Future work
Future work could evaluate ways to combine the techniques, as well as other modelling improvements to increase performance. It would be interesting to evaluate whether code that needs comments is worse code, or code that requires refactoring. Are their models also code smell detectors?

* Day 53: SE
- *Title*: Better Code, Better Sharing: On the Need of Analyzing Jupyter Notebooks
- *Year*: 2020
- *Proc*: ICSE-NIER

The following content is referred from [23]
** Problem
As many of the notebook authors are experts in their scientific fields, but laymen with respect to software engineering, one may ask questions on the quality of notebooks and their code.

** Method
In a preliminary study, they experimentally demonstrate that Jupyter notebooks are inundated with poor quality code, e.g., not respecting recommended coding practices, or containing unused variables and deprecated functions.

** Result
Their experimental results reveal that Jupyter notebooks are indeed inundated with poor coding practices.

** Future work
They argue that the community needs to propose promising approaches to (1) enforce good coding styles, (2) improve the quality and reliability of the code, (3) apply best practices for software quality, and (4) ensure a good balance between text and code in Jupyter notebooks—the more given how many published scientific results depend on calculations made in notebooks.

* Day 54: SE
- *Title*: Assessing Practitioner Beliefs about Software Defect Prediction
- *Year*: 2020
- *Proc*: ICSE-SEIP

The following content is referred from [24]
** Problem
If we do not understand what factors lead to software defects, then that has detrimental effects for quality assurance, trust, insight, training, and tool development.

** Method
They analyze 3 times more changes (commits) than recent defect prediction work [11, 19] and the volume of our dataset is 8 times larger as we expand those changes (commits) that results in 301,627 source code file entries filtered from 524,851 in total.

** Result
Their conclusion will be that we need to change the nature of the debate with Software Engineering. Specifically, while it is important to report the effects that hold right now, it is also important to report on what effects change over time.

** Future work
- They advise focusing on factors that help to answer when & where support for beliefs holds for their future work.
- Other researchers endorse their call for more reasoning about the context in SE.

* Day 55: SE
- *Title*: Skyline: Interactive In-Editor Computational Performance Profiling for Deep Neural Network Training
- *Year*: 2020
- *Proc*: UIST

The following content is referred from [25]
** Problem
However, effectively performing this debugging requires intimate knowledge about the underlying software and hardware systems—something that the typical deep learning developer may not have.

** Method
They present SKYLINE: a new interactive tool for DNN training that supports in-editor computational performance profiling, visualization, and debugging. SKYLINE’s key contribution is that it leverages special computational properties of DNN training to provide (i) interactive performance
predictions and visualizations, and (ii) directly manipulatable
visualizations that, when dragged, mutate the batch size in the code.

** Result
An exploratory qualitative user study of SKYLINE produced promising results; all the participants found SKYLINE to be useful and easy to use.

** Future work
Extensibility
- Supporting other frameworks
- Supporting other editors
- Mutating other model parameters

* Day 56: SE
- *Title*: Suggesting Natural Method Names to Check Name Consistencies
- *Year*: 2020
- *Proc*: ICSE

The following content is referred from [26]
** Problem
Misleading names of the methods in a project or the APIs in a
software library confuse developers about program functionality
and API usages, leading to API misuses and defects.
** Method
They introduce MNire, a machine learning approach to check the
consistency between the name of a given method and its implementation.

The workflow is as follows:
1. MNIRE first generate a candidate name
2. compare the candidate name wth the current name
- if the two names are sufficiently simiar, they consider the method as consistent


three contexts of a method:
1. its body
2. the interface(the method's parameter types and return type)
3. the enclosing class's name

unique idea: treat the name generation as an abstract summarization on tokens collected from the names of the program entities in the three contexts of a method

*** Related work
1. suggest method names
2. recover/predict the names or types of program entities
3. code representations

** Result
- In detecting inconsistency method names, MNire improves the state-of-the-art approach by 10.4% and 11% relatively in recall and precision, respectively. 

- In method name recommendation, MNire improves relatively over the state-of-the-art technique, code2vec, in both recall (18.2% higher) and precision (11.1% higher).

** Future work
work on other program languages other Java

* Day 57: SE
- *Title*: Is Your Quantum Program Bug-Free?
- *Year*: 2020
- *Proc*: ICSE NIER

The following content is referred from [27]
** Problem
As more programmers are starting to look at writing quantum programs, they face an inevitable task of debugging their code. How should the programs for quantum computers be debugged?

** Method
They discuss existing debugging tactics, used in developing programs for classic computers, and show which ones can be readily adopted. They also highlight quantum-computer-specific debugging issues and list novel techniques that are needed to address these issues.

** Result
QC field is rapidly evolving, and the Software Engineering (SE) community should start bringing SE practices into the QC world.

** Future work
This work would be of interest to practitioners, creating quantum programs, as well as researchers, developing the next generations of tooling for QC.

* Day 58: SE
- *Title*: Efficient Generation of Error-Inducing Floating-Point Inputs via Symbolic Execution
- *Year*: 2020
- *Proc*: ICSE

The following content is referred from [28]
** Problem
Floating point is widely used in software to emulate arithmetic over reals. Unfortunately, floating point leads to rounding errors that propagate and accumulate during execution.

** Method
They formulate the problem of generating high error-inducing floating-point inputs as a code coverage maximization problem solved using symbolic execution.

First, they formulated two inaccuracy checks for large precision loss and cancellation. The injection of in-accuracy checks after floating-point computation enables symbolic execution to explore specialized branches that cause numerical in-accuracy, which can lead to large errors in the final result. Second, they proposed optimizations to alleviate path explosion. In partic- ular, this was achieved by strategically reducing the number of symbolic variables via concretization. They implemented our algorithm in a tool named FPGen, and presented an evaluation on 21 numerical programs including matrix computation and statistics libraries.

** Result
The results show that FPGen is able to expose errors for 20 of the evaluated programs while the state-of-the-art error-inducing input generator S3FP only triggers errors for 13 out of 21 programs. Moreover, FPGen triggered an error as large as 10−6on average while the maximum error S3FP triggered is about 10−8on average.

** Future work
1. identify other code areas to inject inaccuracy checks
2. It would be interesting to complement their work using techniques to speedup symbolic execution.

* Day 59: SE
- *Title*: A Study on the Prevalence of Human Values in Software Engineering Publications, 2015 – 2018
- *Year*: 2020
- *Proc*: ICSE

The following content is referred from [29]
** Problem
Failure to account for human values in software (e.g., equality and fairness) can result in user dissatisfaction and negative socioeconomic impact. Engineering these values in software, however, requires technical and methodological support throughout the development life cycle.

** Method
To investigate the prevalence of human values in SE research, they manually classified publications from top-tier SE conferences and journals based on their relevance to different values.

** Result
(a) only a small proportion of the publications directly consider values, classified as relevant publications; (b) for the majority of the values, very few or no relevant publications were found; and (c) The prevalence of the relevant publications was higher in SE conferences compared to SE journals.

(a) only 16% of publications were directly relevant to human values, referred to, henceforth, as relevant publications; (b) for 60% of human values, there were no relevant publications; (c) on average, 2 relevant papers were found per value, while for 79% of values, the number of relevant publications was ≤ 2; and (d) 88% of relevant papers were published in SE conferences rather than journals.

** Future work
1. They would like to extend this study using a machine learning approach. Manually labelled data from this study could be used for training machine learning algorithms to classify larger sets of publications with the aim to better visualize how SE research addresses human values.
2. Theyt also plan to utilise their manually labelled data captured from various SE contexts to develop definitions of human values that are relatively easy for practitioners to understand and implement.
3. Finally, they plan to carry out case studies in software organizations to investigate whether SE research related to human values has actually made an impact on SE practice.

* Day 60: NLP
- *Title*: Pre-trained Models for Natural Language Processing: A Survey
- *Year*: 2020

The following content is referred from [30]

** Two generations
*** word embeddings
*** contextual word embeddings

Two kinds of word embeddings:
1. non-contextual embeddings
2. contextual embeddings

The difference between them is whether the embedding for a word dynamically changes according to the context it appears in.


In NLP, the datasets of most supervised tasks are not large enough to train a good PTM

** Pre-training tasks
1. language modeling
2. mask language modeling
3. permutated language modeling
4. denoising autoencoder
5. contrastive learning
- Deep InfoMax (DIM)
- Replaced Token Detection (RTD)
- Next Sentence Prediction (NSP)
- Sentence Order Prediction (SOP)


RoBERTa improves BERT by dynamic masking
* Reference
1. Meng, Y., Zhang, Y., Huang, J., Zhang, Y., Zhang, C., & Han, J. (2020). Hierarchical Topic Mining via Joint Spherical Tree and Text Embedding. arXiv preprint arXiv:2007.09536.

2. APA is unavailable now

3. Montandon, J. E., Silva, L. L., & Valente, M. T. (2019, May). Identifying experts in software libraries and frameworks among GitHub users. In 2019 IEEE/ACM 16th International Conference on Mining Software Repositories (MSR) (pp. 276-287). IEEE.

4. Xin, J., Tang, R., Lee, J., Yu, Y., & Lin, J. (2020). DeeBERT: Dynamic Early Exiting for Accelerating BERT Inference. arXiv preprint arXiv:2004.12993.

5. Chen, Z., Trabelsi, M., Heflin, J., Xu, Y., & Davison, B. D. (2020). Table Search Using a Deep Contextualized Language Model. arXiv preprint arXiv:2005.09207.

6. An Analysis of BERT in Document Ranking APA is unavailable now

7. Cai, J., Zhu, Z., Nie, P., & Liu, Q. (2020). A Pairwise Probe for Understanding BERT Fine-Tuning on Machine Reading Comprehension. arXiv preprint arXiv:2006.01346.

8. Tao, G., Ma, S., Liu, Y., Xu, Q., & Zhang, X. TRADER: Trace Divergence Analysis and Embedding Regulation for Debugging Recurrent Neural Networks.

9. Ribeiro, M. T., Wu, T., Guestrin, C., & Singh, S. (2020). Beyond Accuracy: Behavioral Testing of NLP Models with CheckList. arXiv preprint arXiv:2005.04118.

10. Shrestha, N., Botta, C., Barik, T., & Parnin, C. (2020, May). Here We Go Again: Why Is It Difficult for Developers to Learn Another Programming Language?. In Proceedings of the 42nd International Conference on Software Engineering, ICSE.

11. Huo, X., Thung, F., Li, M., Lo, D., & Shi, S. T. (2019). Deep transfer bug localization. IEEE Transactions on Software Engineering.

12. Johnson, B., Brun, Y., & Meliou, A. (2020). Causal Testing: Understanding Defects’ Root Causes. In Proceedings of the 2020 International Conference on Software Engineering.

13. Zhong, H., Meng, N., Li, Z., & Jia, L. An Empirical Study on API Parameter Rules.

14. Chen, J., Chen, C., Xing, Z., Xu, X., Zhu, L., Li, G., & Wang, J. (2020). Unblind Your Apps: Predicting Natural-Language Labels for Mobile GUI Components by Deep Learning. arXiv preprint arXiv:2003.00380.

15. Chen, J., Chen, C., Xing, Z., Xia, X., Zhu, L., Grundy, J., & Wang, J. (2020). Wireframe-based UI design search through image autoencoder. ACM Transactions on Software Engineering and Methodology (TOSEM), 29(3), 1-31.

16. Islam, M. J., Pan, R., Nguyen, G., & Rajan, H. (2020). Repairing Deep Neural Networks: Fix Patterns and Challenges. arXiv preprint arXiv:2005.00972.

17. Gao, X., Saha, R. K., Prasad, M. R., & Roychoudhury, A. Fuzz Testing based Data Augmentation to Improve Robustness of Deep Neural Networks.

18. Li, K., Xiang, Z., Chen, T., Wang, S., & Tan, K. C. (2020). Understanding the Automated Parameter Optimization on Transfer Learning for CPDP: An Empirical Study. arXiv preprint arXiv:2002.03148.

19. Zhai, Juan, et al. "CPC: Automatically classifying and propagating natural language comments via program analysis." (2019).

20. Rahman, A., Farhana, E., Parnin, C., & Williams, L. (2020, May). Gang of eight: A defect taxonomy for infrastructure as code scripts. In Proceedings of the 42nd International Conference on Software Engineering, ICSE (Vol. 20).

21. Yang, Y., Xia, X., Lo, D., Bi, T., Grundy, J., & Yang, X. (2020). Predictive Models in Software Engineering: Challenges and Opportunities. arXiv preprint arXiv:2008.03656.

22. Louis, A., Dash, S. K., Barr, E. T., Ernst, M. D., & Sutton, C. (2020). Where should I comment my code? A dataset and model for predicting locations that need comments. In Proceedings of the 42nd International Conference on Software Engineering (New Ideas and Emerging Results)(ICSE NIER 2020). Association for Computing Machinery (ACM).

23. Wang, J., Li, L., & Zeller, A. (2019). Better Code, Better Sharing: On the Need of Analyzing Jupyter Notebooks. arXiv preprint arXiv:1906.05234.

24. Shrikanth, N. C., & Menzies, T. (2019). Assessing Practitioner Beliefs about Software Defect Prediction. arXiv, arXiv-1912.

25. APA unavailable

26. Nguyen, S., Phan, H., Le, T., & Nguyen, T. N. Suggesting Natural Method Names to Check Name Consistencies.

27. Miranskyy, A., Zhang, L., & Doliskani, J. (2020). Is Your Quantum Program Bug-Free?. arXiv preprint arXiv:2001.10870.

28. Guo, Hui, and Cindy Rubio-González. "Efficient Generation of Error-Inducing Floating-Point Inputs via Symbolic Execution." (2019).

29. Perera, H., Nurwidyantoro, A., Hussain, W., Mougouei, D., Whittle, J., Shams, R. A., & Oliver, G. (2019). A study on the prevalence of human values in software engineering publications, 2015-2018. arXiv preprint arXiv:1907.07874.

30. Qiu, X., Sun, T., Xu, Y., Shao, Y., Dai, N., & Huang, X. (2020). Pre-trained models for natural language processing: A survey. arXiv preprint arXiv:2003.08271.