* Duration
From 26 July 2020 to 
* Content
1. [[#day-31-dm][Day 31: Hierarchical Topic Mining via Joint Spherical Tree and Text Embedding]]
2. [[#day-32-se][Day 32: Simulee: Detecting CUDA Synchronization Bugs via Memory-Access Modeling]]
3. [[#day-33-se][Day 33: Identifying experts in software libraries and frameworks among GitHub users]]
4. [[#day-34-nlp][Day 34: DeeBERT: Dynamic Early Exiting for Accelerating BERT Inference]]
5. [[#day-35-ir][Day 35: Table Search Using a Deep Contextualized Language Model]]
6. [[#day-36-ir][Day 36: An Analysis of BERT in Document Ranking]]
7. [[#day-37-ir][Day 38: A Pairwise Probe for Understanding BERT Fine-Tuning on Machine Reading Comprehension]]
8. [[#day-38-se][Day 39: TRADER: Trace Divergence Analysis and Embedding Regulation for Debugging Recurrent Neural Networks]]

* Day 31: DM
- *Title*: Hierarchical Topic Mining via Joint Spherical Tree and Text Embedding
- *Year*: 2020
- *Proc*: KDD

The following content is referred from [1]
** Keywords
Topic Mining; Topic Hierarchy; Text Embedding; Tree Embedding
** Problem
They propose a new task, *Hierarchical Topic Mining*, which takes only a topic hierarchy described by category names as user guidance, and aims to retrieve a set of coherent and representative terms under each category to help users comprehend his/her interested topics.

** Method
Hierarchical Topic Mining is weakly-supervised as it requires the user to provide the names of the hierarchy categories which serve as the minimal supervision and focuses on retrieving representative terms only for the provided categories.

** Result
The proposed model, named JoSH, mines a high-quality set of hierarchical topics with high efficiency and benefits weakly-supervised hierarchical text classification tasks.

** Future work
- One can extend JoSH to not only focus on a user-given category structure, but also be able to discover other latent topics from a text corpus, probably by relaxing the assumption that a document is generated from one ofthe given topics or collaborating with other taxonomy construction algorithms.

- Embedding tree or graph structures along with textual data in the spherical space for mining structured knowledge from text corpora.

* Day 32: SE
- *Title*: Simulee: Detecting CUDA Synchronization Bugs via Memory-Access Modeling
- *Year*: 2020
- *Proc*: ICSE

The following content is referred from [2]
** Problem
How to effectively and efficiently detect CUDA synchronization bugs remains a challenging open problem.

** Method
They pro-pose the first lightweight CUDA synchronization bug detection framework, namely Simulee, to model CUDA program execution by interpreting the corresponding LLVM bytecode and collecting the memory-access information for automatically detecting general CUDA synchronization bugs.

** Result
Simulee can detect 21 out of the 24 manually identified bugs in our preliminary study and also 24 previously unknown bugs among all projects, 10 of which have already been confirmed by the developers.

The results suggest that Simulee is able to detect most of the manually identified synchronization bugs in the benchmark.

* Day 33: SE
- *Title*: Identifying experts in software libraries and frameworks among GitHub users

- *Year*: 2019
- *Proc*: MSR

The following content is referred from [3]
** Problem
We still lack techniques to assess developers expertise in widely popular libraries and frameworks.

** Method
They evaluate the performance of unsupervised (based on clustering) and supervised machine learning classifiers (Random Forest and SVM) to identify experts in three popular JavaScript libraries: facebook/react, mongodb/node-mongodb, and socketio/socket.io.

** Result
First, they found that standard machine learning classifiers (e.g., Random Forest and SVM) do not have a good performance in this problem, at least when they are trained with all developers from a sample of GitHub users. The main reason is that not all experts have a strong presence on GitHub. By contrast, they used clustering techniques to identify experts with high activity on GitHub projects that depend on particular libraries and frameworks. Particularly, they found clusters with 74% (REACT), 65% (NODE-MONGODB), and 75% (SOCKET.IO) of experts.

** Future work
(1) investigate other target libraries and frameworks; 
(2) investigate the use of features from other platforms, such as Stack Overflow and TopCoder;
(3) investigate the accuracy of the proposed method with other developers, including developers of less popular projects

* Day 34: NLP
- *Title*: DeeBERT: Dynamic Early Exiting for Accelerating BERT Inference
- *Year*: 2020
- *Proc*: NLP

The following content is referred from [4]

** Problem
Large-scale pre-trained language models are slow in inference.

** Method
They propose DeeBERT (Dynamic early exiting for BERT) to accelerate BERT.

The inspiration comes from a well-known observation in the computer vision community: in deep convolutional neural networks, higher layers typically produce more detailed and finer-grained features.

DeeBERT accelerates BERT inference by inserting extra classification layers (which we refer to as off-ramps) between each transformer layer of BERT.

There is no early stopping and the checkpoint after full fine-tuning is chosen.

** Result
They conduct experiments on BERT and RoBERTa with six GLUE datasets, showing that DeeBERT is capable of accelerating model inference by up to ∼40% with minimal model quality degradation on downstream tasks.

DeeBERT, an effective method that exploits redundancy in BERT models to achieve better quality–efficiency trade-offs.

** Future work
(1) DeeBERT’s training method, while maintaining good quality in the last off-ramp, reduces model capacity available for intermediate off-ramps; it would be important to look for a method that achieves a better balance between all off-ramps.

(2) The reasons why some transformer layers appear redundant2 and why DeeBERT considers some samples easier than others remain unknown; it would be interesting to further explore relationships between pre-training and layer redundancy, sample complexity and exit layer, and related characteristics.

* Day 35: IR
- *Title*: Table Search Using a Deep Contextualized Language Model
- *Year*: 2020
- *Proc*: SIGIR

The following content is referred from [5]
** Problem
They consider the task ofad hoc table retrieval where given a keyword query, a list of ranked tables are returned.

They use the deep contextualized language model BERT for the task of ad hoc table retrieval. They investigate how to encode table content considering the table structure and input length limit of BERT. We also propose an approach that incorporates features from prior literature on table retrieval and jointly trains them with BERT.

** Method
In experiments on public datasets, they show that their best approach can outperform the previous state-of-the-art method and BERT baselines with a large margin under different evaluation metrics.

** Result
Our proposed Hybrid-BERT-Row-Max method outperforms the previous state-of-the-art and BERT baselines with a large margin on WikiTables dataset.

** Future work
Future work could design a framework that automatically chooses the strategy considering the query types. Besides, designing pretraining tasks for tables and pretraining BERT on a large table collection could be promising to further improve the performance of BERT on table-related tasks such as table retrieval.

* Day 36: IR
- *Title*: An Analysis of BERT in Document Ranking
- *Year*: 2020
- *Proc*: SIGIR

The following content is referred from [6]
** Problem
To increase the explainability of the ranking process performed by BERT, we investigate a state-of-the-art BERT-based ranking model with focus on its attention mechanism and interaction behavior.

They believe this baseline is too simple, so whether and how BERT can learn good representations for queries and documents is not thoroughly investigated.

** Method
First, an attribution technique is used to study the token importance in different layers. 

Second, several probing classifiers are trained to study the relevance signal carried by the token representations. 

Third, they compare the performance of BERT when its attention matrix is masked in different ways to investigate the importance of interactions.

** Result
It demonstrates that BERT extracts query-independent representations for document. Thus, the representations ofdocument tokens can be pre-calculated offline to improve efficiency.

** Future work
Transforming BERT to a more efficient representation-focused model

* Day 37: IR
- *Title*: A Pairwise Probe for Understanding BERT Fine-Tuning on Machine Reading Comprehension
- *Year*: 2020
- *Proc*: SIGIR

The following content is referred from [7]
** Problem
In this paper, inspired by the observation that most probing tasks involve identifying matched pairs of phrases (e.g. coreference requires matching an entity and a pronoun), they propose a pairwise probe to understand BERT fine-tuning on the machine reading comprehension (MRC) task.

** Method
In order to probe the above phenomena, we design a pairwise ranking metric to quantitatively compare pre-trained and fine-tuned model with in-domain data. The metric is designed to measure whether matching pairs are closer than random un-matching pairs that aim to provide insight about how well related information are encoded.

** Result
(1) Fine-tuning has little effect on the fundamental and low-level information and general semantic tasks. 
(2) For specific abilities required for downstream tasks, fine-tuned BERT is better than pre-trained BERT and such gaps are obvious after the fifth layer

** Future work
One can apply the pairwise ranking metric to analyze impact of fine-tuning on other tasks.

* Day 39: SE
- *Title*: TRADER: Trace Divergence Analysis and Embedding Regulation for Debugging Recurrent Neural Networks
- *Year*: 2020
- *Proc*: ICSE

 The following content is referred from [8]
** Problem
They propose a new technique to automatically diagnose how problematic embeddings impact model performance, by comparing model execution traces from correctly and incorrectly executed samples.

** Method
They focus on debugging RNN models for textual inputs (e.g., sentiment analysis for developer comments), especially for a type of bugs in which problematic word embeddings lead to suboptimal model accuracy.

** Result
The experiments show that TRADER can consistently and effectively improve accuracy for real world models and datasets by 5.37% on average, which represents substantial improvement in the literature of RNN models.

* Reference
1. Meng, Y., Zhang, Y., Huang, J., Zhang, Y., Zhang, C., & Han, J. (2020). Hierarchical Topic Mining via Joint Spherical Tree and Text Embedding. arXiv preprint arXiv:2007.09536.

2. APA is unavailable now

3. Montandon, J. E., Silva, L. L., & Valente, M. T. (2019, May). Identifying experts in software libraries and frameworks among GitHub users. In 2019 IEEE/ACM 16th International Conference on Mining Software Repositories (MSR) (pp. 276-287). IEEE.

4. Xin, J., Tang, R., Lee, J., Yu, Y., & Lin, J. (2020). DeeBERT: Dynamic Early Exiting for Accelerating BERT Inference. arXiv preprint arXiv:2004.12993.

5. Chen, Z., Trabelsi, M., Heflin, J., Xu, Y., & Davison, B. D. (2020). Table Search Using a Deep Contextualized Language Model. arXiv preprint arXiv:2005.09207.

6. An Analysis of BERT in Document Ranking APA is unavailable now

7. Cai, J., Zhu, Z., Nie, P., & Liu, Q. (2020). A Pairwise Probe for Understanding BERT Fine-Tuning on Machine Reading Comprehension. arXiv preprint arXiv:2006.01346.

8. Tao, G., Ma, S., Liu, Y., Xu, Q., & Zhang, X. TRADER: Trace Divergence Analysis and Embedding Regulation for Debugging Recurrent Neural Networks.