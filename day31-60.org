* Duration
From 26 July 2020 to 
* Content
1. [[#day-31-dm][Day 31: Hierarchical Topic Mining via Joint Spherical Tree and Text Embedding]]
2. [[#day-32-se][Day 32: Simulee: Detecting CUDA Synchronization Bugs via Memory-Access Modeling]]
3. [[#day-33-se][Day 33: Identifying experts in software libraries and frameworks among GitHub users]]
4. [[#day-34-nlp][Day 34: DeeBERT: Dynamic Early Exiting for Accelerating BERT Inference]]
5. [[#day-35-ir][Day 35: Table Search Using a Deep Contextualized Language Model]]
6. [[#day-36-ir][Day 36: An Analysis of BERT in Document Ranking]]
7. [[#day-37-ir][Day 37: A Pairwise Probe for Understanding BERT Fine-Tuning on Machine Reading Comprehension]]
8. [[#day-38-se][Day 38: TRADER: Trace Divergence Analysis and Embedding Regulation for Debugging Recurrent Neural Networks]]
9. [[#day-39-nlp][Day 39: Beyond Accuracy: Behavioral Testing of NLP Models with CheckList]]
10. [[#day-40-se][Day 40: Here We Go Again: Why Is It Difficult for Developers to Learn Another Programming Language?]]
11. [[#day-41-se][Day 41: Deep Transfer Bug Localization]]
12. [[#day-42-se][Day 42: Causal Testing: Understanding Defects’ Root Causes]]
13. [[#day-43-se][Day 43: An Empirical Study on API Parameter Rules Hao]]
14. [[#day-44-se][Day 44: Unblind Your Apps: Predicting Natural-Language Labels for Mobile GUI Components by Deep Learning]]
15. [[#day-45-se][Day 45: Wireframe-based UI Design Search through Image Autoencoder]]
16. [[#day-46-se][Day 46: Repairing Deep Neural Networks: Fix Patterns and Challenges]]
17. [[#day-47-se][Day 47: Fuzz Testing based Data Augmentation to Improve Robustness of Deep Neural Networks]]
18. [[#day-48-se][Day 48: Understanding the Automated Parameter Optimization on Transfer Learning for CPDP: An Empirical Study]]

* Day 31: DM
- *Title*: Hierarchical Topic Mining via Joint Spherical Tree and Text Embedding
- *Year*: 2020
- *Proc*: KDD

The following content is referred from [1]
** Keywords
Topic Mining; Topic Hierarchy; Text Embedding; Tree Embedding
** Problem
They propose a new task, *Hierarchical Topic Mining*, which takes only a topic hierarchy described by category names as user guidance, and aims to retrieve a set of coherent and representative terms under each category to help users comprehend his/her interested topics.

** Method
Hierarchical Topic Mining is weakly-supervised as it requires the user to provide the names of the hierarchy categories which serve as the minimal supervision and focuses on retrieving representative terms only for the provided categories.

** Result
The proposed model, named JoSH, mines a high-quality set of hierarchical topics with high efficiency and benefits weakly-supervised hierarchical text classification tasks.

** Future work
- One can extend JoSH to not only focus on a user-given category structure, but also be able to discover other latent topics from a text corpus, probably by relaxing the assumption that a document is generated from one ofthe given topics or collaborating with other taxonomy construction algorithms.

- Embedding tree or graph structures along with textual data in the spherical space for mining structured knowledge from text corpora.

* Day 32: SE
- *Title*: Simulee: Detecting CUDA Synchronization Bugs via Memory-Access Modeling
- *Year*: 2020
- *Proc*: ICSE

The following content is referred from [2]
** Problem
How to effectively and efficiently detect CUDA synchronization bugs remains a challenging open problem.

** Method
They pro-pose the first lightweight CUDA synchronization bug detection framework, namely Simulee, to model CUDA program execution by interpreting the corresponding LLVM bytecode and collecting the memory-access information for automatically detecting general CUDA synchronization bugs.

** Result
Simulee can detect 21 out of the 24 manually identified bugs in our preliminary study and also 24 previously unknown bugs among all projects, 10 of which have already been confirmed by the developers.

The results suggest that Simulee is able to detect most of the manually identified synchronization bugs in the benchmark.

* Day 33: SE
- *Title*: Identifying experts in software libraries and frameworks among GitHub users

- *Year*: 2019
- *Proc*: MSR

The following content is referred from [3]
** Problem
We still lack techniques to assess developers expertise in widely popular libraries and frameworks.

** Method
They evaluate the performance of unsupervised (based on clustering) and supervised machine learning classifiers (Random Forest and SVM) to identify experts in three popular JavaScript libraries: facebook/react, mongodb/node-mongodb, and socketio/socket.io.

** Result
First, they found that standard machine learning classifiers (e.g., Random Forest and SVM) do not have a good performance in this problem, at least when they are trained with all developers from a sample of GitHub users. The main reason is that not all experts have a strong presence on GitHub. By contrast, they used clustering techniques to identify experts with high activity on GitHub projects that depend on particular libraries and frameworks. Particularly, they found clusters with 74% (REACT), 65% (NODE-MONGODB), and 75% (SOCKET.IO) of experts.

** Future work
(1) investigate other target libraries and frameworks; 
(2) investigate the use of features from other platforms, such as Stack Overflow and TopCoder;
(3) investigate the accuracy of the proposed method with other developers, including developers of less popular projects

* Day 34: NLP
- *Title*: DeeBERT: Dynamic Early Exiting for Accelerating BERT Inference
- *Year*: 2020
- *Proc*: NLP

The following content is referred from [4]

** Problem
Large-scale pre-trained language models are slow in inference.

** Method
They propose DeeBERT (Dynamic early exiting for BERT) to accelerate BERT.

The inspiration comes from a well-known observation in the computer vision community: in deep convolutional neural networks, higher layers typically produce more detailed and finer-grained features.

DeeBERT accelerates BERT inference by inserting extra classification layers (which we refer to as off-ramps) between each transformer layer of BERT.

There is no early stopping and the checkpoint after full fine-tuning is chosen.

** Result
They conduct experiments on BERT and RoBERTa with six GLUE datasets, showing that DeeBERT is capable of accelerating model inference by up to ∼40% with minimal model quality degradation on downstream tasks.

DeeBERT, an effective method that exploits redundancy in BERT models to achieve better quality–efficiency trade-offs.

** Future work
(1) DeeBERT’s training method, while maintaining good quality in the last off-ramp, reduces model capacity available for intermediate off-ramps; it would be important to look for a method that achieves a better balance between all off-ramps.

(2) The reasons why some transformer layers appear redundant2 and why DeeBERT considers some samples easier than others remain unknown; it would be interesting to further explore relationships between pre-training and layer redundancy, sample complexity and exit layer, and related characteristics.

* Day 35: IR
- *Title*: Table Search Using a Deep Contextualized Language Model
- *Year*: 2020
- *Proc*: SIGIR

The following content is referred from [5]
** Problem
They consider the task ofad hoc table retrieval where given a keyword query, a list of ranked tables are returned.

They use the deep contextualized language model BERT for the task of ad hoc table retrieval. They investigate how to encode table content considering the table structure and input length limit of BERT. We also propose an approach that incorporates features from prior literature on table retrieval and jointly trains them with BERT.

** Method
In experiments on public datasets, they show that their best approach can outperform the previous state-of-the-art method and BERT baselines with a large margin under different evaluation metrics.

** Result
Our proposed Hybrid-BERT-Row-Max method outperforms the previous state-of-the-art and BERT baselines with a large margin on WikiTables dataset.

** Future work
Future work could design a framework that automatically chooses the strategy considering the query types. Besides, designing pretraining tasks for tables and pretraining BERT on a large table collection could be promising to further improve the performance of BERT on table-related tasks such as table retrieval.

* Day 36: IR
- *Title*: An Analysis of BERT in Document Ranking
- *Year*: 2020
- *Proc*: SIGIR

The following content is referred from [6]
** Problem
To increase the explainability of the ranking process performed by BERT, we investigate a state-of-the-art BERT-based ranking model with focus on its attention mechanism and interaction behavior.

They believe this baseline is too simple, so whether and how BERT can learn good representations for queries and documents is not thoroughly investigated.

** Method
First, an attribution technique is used to study the token importance in different layers. 

Second, several probing classifiers are trained to study the relevance signal carried by the token representations. 

Third, they compare the performance of BERT when its attention matrix is masked in different ways to investigate the importance of interactions.

** Result
It demonstrates that BERT extracts query-independent representations for document. Thus, the representations ofdocument tokens can be pre-calculated offline to improve efficiency.

** Future work
Transforming BERT to a more efficient representation-focused model

* Day 37: IR
- *Title*: A Pairwise Probe for Understanding BERT Fine-Tuning on Machine Reading Comprehension
- *Year*: 2020
- *Proc*: SIGIR

The following content is referred from [7]
** Problem
In this paper, inspired by the observation that most probing tasks involve identifying matched pairs of phrases (e.g. coreference requires matching an entity and a pronoun), they propose a pairwise probe to understand BERT fine-tuning on the machine reading comprehension (MRC) task.

** Method
In order to probe the above phenomena, we design a pairwise ranking metric to quantitatively compare pre-trained and fine-tuned model with in-domain data. The metric is designed to measure whether matching pairs are closer than random un-matching pairs that aim to provide insight about how well related information are encoded.

** Result
(1) Fine-tuning has little effect on the fundamental and low-level information and general semantic tasks. 
(2) For specific abilities required for downstream tasks, fine-tuned BERT is better than pre-trained BERT and such gaps are obvious after the fifth layer

** Future work
One can apply the pairwise ranking metric to analyze impact of fine-tuning on other tasks.

* Day 38: SE
- *Title*: TRADER: Trace Divergence Analysis and Embedding Regulation for Debugging Recurrent Neural Networks
- *Year*: 2020
- *Proc*: ICSE

The following content is referred from [8]
** Problem
They propose a new technique to automatically diagnose how problematic embeddings impact model performance, by comparing model execution traces from correctly and incorrectly executed samples.

** Method
They focus on debugging RNN models for textual inputs (e.g., sentiment analysis for developer comments), especially for a type of bugs in which problematic word embeddings lead to suboptimal model accuracy.

** Result
The experiments show that TRADER can consistently and effectively improve accuracy for real world models and datasets by 5.37% on average, which represents substantial improvement in the literature of RNN models.

* Day 39: NLP
- *Title*: Beyond Accuracy: Behavioral Testing of NLP Models with CheckList
- *Year*: 2020
- *Proc*: ACL

The following content is referred from [9]
** Problem
While useful, accuracy on benchmarks is not sufficient for evaluating NLP models.

** Method
They introduce CheckList, a task-agnostic methodology for testing NLP models

CheckList includes a matrix of general linguistic capabilities and test types that facilitate comprehensive test ideation, as well as a software tool to generate a large and diverse number of test cases quickly. 

** Result
They illustrate the utility of CheckList with tests for three tasks, identifying critical failures in both commercial and state-of-art models. In a user study, a team responsible for a commercial sentiment analysis model found new and actionable bugs in an extensively tested model. In another user study, NLP practitioners with CheckList created twice as many tests, and found almost three times as many bugs as users without it.

* Day 40: SE
- *Title*: Here We Go Again: Why Is It Difficult for Developers to Learn Another Programming Language?
- *Year*: 2020
- *Proc*: SE

The following content is referred from [10]
** Problem
To understand if programmers have difficulty learning additional programming languages, they conducted an empirical study of Stack Overflow questions across 18 different programming languages.

** Method
They hypothesized that previous knowledge could potentially interfere with learning a new programming language. From their inspection of 450 Stack Overflow questions, they found 276 instances of interference that occurred due to faulty assumptions originating from knowledge about a different language.

They analyzed 450 posts for 18 different programming languages and qualitatively coded each post, characterizing posts in terms of whether or not programmers made incorrect assumptions based on their previous programming knowledge. Then, to understand what learning strategies programmers used when learning another language and why previous knowledge could interfere with this process. They interviewed 16 professional programmers who had recently switched to a new programming language.

** Result
- Cross-language interference is a problem: 276 (61%) cross-language posts on Stack Overflow contained incorrect assumptions due to interference with previous language knowledge.

- Based on our interviews, professional programmers primarily learned new languages on their own, using an opportunistic strategy that often involved relating the new language to previous language knowledge; however, this results in interference which harms their learning.

- Learning a new language involves breaking down old habits, shifting one’s mindset, dealing with little-to-no mapping to previous languages, searching for proper documentation, and retooling in a new environment. All together, these challenges make learning another language difficult.

* Day 41: SE
- *Title*: Deep Transfer Bug Localization Xuan
- *Year*: 2019
- *Journal*: TSE

The following content is referred from [11]
** Problem
Sufficient bug data is often unavailable for many projects and companies. This raises the need for cross-project bug localization – the use of data from a project to help locate bugs in another project

** Method
They propose a deep transfer learning approach for cross-project bug localization. The proposed approach named TRANP-CNN extracts transferable semantic features from source project and fully exploits labeled data from target project for effective cross-project bug localization.

They proposed a novel deep transfer neural network named TRANP-CNN (TRAnsfer Natural and Program Language Convolutional Neural Network). Firstly, TRANP-CNN takes bug reports and source files as inputs and learns a common transferable latent feature representation shared by both source and target projects. Next, TRANP-CNN creates a pair of prediction functions that are biased towards the source and target projects, based on the shared feature representation.

TRANP-CNN consists of four layers: input layer, transferable feature extraction layer, project-specific prediction layer and output layer.

** Result
TRANP-CNN can locate buggy files correctly at top 1, top 5, and top 10 positions for 29.9%, 51.7%, 61.3% of the bugs respectively, which significantly outperform state-of-the-art bug localization solution based on deep learning and several other advanced alternative solutions considering various standard evaluation metrics.

** Future work
They plan to extend the evaluation of TRANP-CNN by including more bug reports from additional projects. They also plan to develop our solution into a tool that is integrated with an IDE followed by its evaluation from industry partners.

* Day 42: SE
- *Title*: Causal Testing: Understanding Defects’ Root Causes
- *Year*: 2020
- *Proc*: ICSE

The following content is referred from [12]
** Problem
Debugging and understanding software behavior is an important part of building software systems. To address this shortcoming of modern debugging tools, this
paper presents CausalTesting, a novel technique for identifying root causes of failing executions based on the theory of counterfactual causality.

** Method
To address this shortcoming of modern debugging tools, this paper presents CausalTesting, a novel technique for identifying root causes of failing executions based on the theory of counterfactual causality.

** Result
Using the Defects4J benchmark, we find that Causal Testing could be applied to 71% of real-world defects, and for 77% of those, it can help developers identify the root cause of the defect.

** Future work
Future work could extend Causal Testing to include oracle mutation. A fruitful line of research, when specifications, formal or informal, are available, is to extract oracles from those specifications.

* Day 43: SE
- *Title*: An Empirical Study on API Parameter Rules Hao
- *Year*: 2020
- *Proc*: ICSE

The following content is referred from [13]
** Problem
API libraries have been widely used, but are often poorly documented. When programmers do not fully understand API usage, they can introduce API-related bugs into their code. To handle this issue, researchers have proposed various approaches to facilitate better API usage. In particular, a popular research area is to mine parameter rules for APIs. To help developers correctly use library APIs, researchers built tools to mine API parameter rules. However, it is still unknown (1) what types of parameter rules there are, and (2) how these rules distribute inside documents and source files.

** Method
They conducted an empirical study to investigate the above-mentioned questions. To analyze as many parameter rules as possible, they took a hybrid approach that combines automatic localization of constrained parameters with manual inspection.

The automatic approach—PaRu—locates parameters that have constraints either documented in Javadoc (i.e., document rules) or implied by source code (i.e., code rules). Our manual inspection (1) identifies and categorizes rules for the located parameters, and (2) establishes mapping between document and code rules. By applying PaRu to 9 widely used libraries, we located 5,334 parameters with either document or code rules. Interestingly, there are only 187 parameters that have both types of rules, and 79 pairs of these parameter rules are unmatched. Additionally, PaRu extracted 1,688 rule sentences from Javadoc and code. We manually classified these sentences into six categories, two of which are overlooked by prior approaches.

** Result
We found that 86.2% of parameters have only code rules; 10.3% of parameters have only document rules; and only 3.5% of parameters have both document and code rules.

** Future work
work towards better mining and recommendation techniques for parameter rules

* Day 44: SE
- *Title*: Unblind Your Apps: Predicting Natural-Language Labels for Mobile GUI Components by Deep Learning
- *Year*: 2020
- *Prco*: ICSE

The following content is referred from [14]
** Problem
The prerequisite of using screen readers is that developers have to add natural-language labels to the image-based components when they are developing the app. Unfortunately, more than 77% apps have issues of missing labels, according to their analysis of 10,408 Android apps.

** Method
To overcome those challenges, they develop a deep learning based
model to automatically predict the content description.

Inspired by image captioning, they adopt the CNN and transformer
encoder decoder for predicting the labels based on the large-scale dataset.
** Result
The experiments show that our LabelDroid can achieve 60.7% exact match and 0.654 ROUGE-L score which outperforms both state-of-the-art baselines. We also demonstrate that the predictions from our model is of higher quality than that from junior Android developers.
** Future work
In the future, they will first improve our model for achieving better quality by taking the app metadata into the consideration. Second, they will also try to test the quality of existing labels by checking if the description is concise and informative.

* Day 45: SE
- *Title*: Wireframe-based UI Design Search through Image Autoencoder
- *Year*: 2020
- *Journal*: TOSEM

The following content is referred from [15]
** Problem
Existing keyword-based, image-similarity-based, and component-matching-based methods cannot reliably find relevant high-fidelity UI designs in a large database alike to the UI wireframe that the developers sketch, in face ofthe great variations in UI designs.

** Method
The key innovation of their search engine is to train a wireframe image autoencoder using a large database of real-application UI designs, without the need for labeling relevant UI designs.

** Result
Our experiments confirm the superior performance of our search engine over existing image-similarity or component-matching-based methods and demonstrate the usefulness of their search engine in real-world UI design tasks.

** Future work
One can extend of the tool to collecting UI elements in WebView components and in specific engine.

* Day 46: SE
- *Title*: Repairing Deep Neural Networks: Fix Patterns and Challenges
- *Year*: 2020
- *Proc*: ICSE

The following content is referred from [16]
** Problem
A significant SE problem in the software that uses DNNs is the
presence of bugs. What are the common bugs in such software? How do they differ? Answering these questions has the potential to fuel SE research on bug detection and repair for DNNs. This work focuses on bug fix patterns.

** Method
They have studied 415 repairs from Stack Overflow and 555 repairs from GitHub for five popular deep learning libraries Caffe, Keras, Tensorflow, Theano, and Torch to understand challenges in repairs and bug repair patterns.

** Result
Their key findings reveal that DNN bug fix patterns are distinctive compared to traditional bug fix patterns; the most common bug fix patterns are fixing data dimension and neural network connectivity; DNN bug fixes have the potential to introduce adversarial vulnerabilities; DNN bug fixes frequently introduce new bugs; and DNN bug localization, reuse of trained model, and coping with frequent releases are major challenges faced by developers when fixing bugs. We also contribute a benchmark of 667 DNN (bug, repair) instances.

** Future work
First and perhaps most immediately, a number of bug fix patterns identified by this work can be automated in repair tools. Such tools for bug repairs can help the developers integrating DNN into their software. Second, an abstract representation of the DNN along with the code that uses it can be developed. We saw several bug fix patterns that rely on analyzing such a representation. Third, there is a critical need to improve bug localization for DNN by addressing unique challenges that arise, and by creating DNN-aware bug localization tools. Fourth, there is an urgent need to detect bugs introduced by dimension mismatch and specially changes that have the potential to introduce vulnerabilities in the DNNs. Fifth, urgent work is needed on upgrade tools that encode the semantics of version changes and keep up with the change in the signature and semantics of DNN libraries.

* Day 47: SE
- *Title*: Fuzz Testing based Data Augmentation to Improve Robustness of Deep Neural Networks
- *Year*: 2020
- *Proc*: ICSE

The following content is referred from [17]
** Problem
Deep neural networks (DNN) have been shown to be notoriously brittle to small perturbations in their input data. This problem is analogous to the over-fitting problem in test-based program synthesis and automatic program repair, which is a consequence of the incomplete specification, i.e., the limited tests or training examples, that the program synthesis or repair algorithm has to learn from.

** Method
They propose a technique that re-purposes software testing methods, specifically mutation-based fuzzing, to augment the training data of DNNs, with the objective of enhancing their robustness. Our technique casts the DNN data augmentation problem as an optimization problem. It uses genetic search to generate the most suitable variant of an input data to use for training the DNN.

They propose a new algorithm that uses guided test generation techniques to address the data aug- mentation problem for robust generalization of DNNs under natural environmental variations. Specifically, we cast data augmentation problem as an optimization problem, and use genetic search on a space of the natural environmental variants of each training input data, to identify the worst variant for augmentation.
** Result
Our evaluation shows that Sensei can improve the robust accuracy of the DNN, compared to the state of the art, on each of the 15 models, by upto 11.9% and 5.5% on average. Further, Sensei-SA can reduce the average DNN training time by 25%, while still improving robust accuracy.

** Future work
Consider combination of two approaches, theirs and others.

* Day 48: SE
- *Title*: Understanding the Automated Parameter Optimization on Transfer Learning for CPDP: An Empirical Study
- *Year*: 2020
- *Proc*: ICSE

The following content is referred from [18]
** Problem
Most CPDP techniques involve two major steps, i.e., transfer learning and classification, each of which has at least one parameter to be tuned to achieve their optimal performance. This practice fits well with the purpose of automated parameter optimization. However, there is a lack of thorough understanding about what are the impacts of automated parameter optimization on various CPDP techniques.

** Method
They present the first empirical study that looks into such impacts on 62 CPDP techniques, 13 of which are chosen from the existing CPDP literature while the other 49 ones have not been explored before.

** Result
(1) Automated parameter optimization substantially improves the defect prediction performance of 77% CPDP techniques with a manageable computational cost.
(2) Transfer learning is of ultimate importance in CPDP.
(3) The research on CPDP is far from mature where it is 'not difficult' to find a better alternative by making a combination of existing transfer learning and classification techniques.

** Future work
One can design sophisticated optimizer for CPDP that explicitly searches the parameter space for the transfer learning part. Furthermore, the problem of portfolio optimization for CPDP, which involves both the selection of combination and parameter tuning, is also one of our ongoing research directions.

Future work should target a whole portfolio of optimization, tuning not only the parameters, but also the algorithmic components, i.e., the selection of appropriate transfer learning and classifier pair, of a CPDP model.

* Reference
1. Meng, Y., Zhang, Y., Huang, J., Zhang, Y., Zhang, C., & Han, J. (2020). Hierarchical Topic Mining via Joint Spherical Tree and Text Embedding. arXiv preprint arXiv:2007.09536.

2. APA is unavailable now

3. Montandon, J. E., Silva, L. L., & Valente, M. T. (2019, May). Identifying experts in software libraries and frameworks among GitHub users. In 2019 IEEE/ACM 16th International Conference on Mining Software Repositories (MSR) (pp. 276-287). IEEE.

4. Xin, J., Tang, R., Lee, J., Yu, Y., & Lin, J. (2020). DeeBERT: Dynamic Early Exiting for Accelerating BERT Inference. arXiv preprint arXiv:2004.12993.

5. Chen, Z., Trabelsi, M., Heflin, J., Xu, Y., & Davison, B. D. (2020). Table Search Using a Deep Contextualized Language Model. arXiv preprint arXiv:2005.09207.

6. An Analysis of BERT in Document Ranking APA is unavailable now

7. Cai, J., Zhu, Z., Nie, P., & Liu, Q. (2020). A Pairwise Probe for Understanding BERT Fine-Tuning on Machine Reading Comprehension. arXiv preprint arXiv:2006.01346.

8. Tao, G., Ma, S., Liu, Y., Xu, Q., & Zhang, X. TRADER: Trace Divergence Analysis and Embedding Regulation for Debugging Recurrent Neural Networks.

9. Ribeiro, M. T., Wu, T., Guestrin, C., & Singh, S. (2020). Beyond Accuracy: Behavioral Testing of NLP Models with CheckList. arXiv preprint arXiv:2005.04118.

10. Shrestha, N., Botta, C., Barik, T., & Parnin, C. (2020, May). Here We Go Again: Why Is It Difficult for Developers to Learn Another Programming Language?. In Proceedings of the 42nd International Conference on Software Engineering, ICSE.

11. Huo, X., Thung, F., Li, M., Lo, D., & Shi, S. T. (2019). Deep transfer bug localization. IEEE Transactions on Software Engineering.

12. Johnson, B., Brun, Y., & Meliou, A. (2020). Causal Testing: Understanding Defects’ Root Causes. In Proceedings of the 2020 International Conference on Software Engineering.

13. Zhong, H., Meng, N., Li, Z., & Jia, L. An Empirical Study on API Parameter Rules.

14. Chen, J., Chen, C., Xing, Z., Xu, X., Zhu, L., Li, G., & Wang, J. (2020). Unblind Your Apps: Predicting Natural-Language Labels for Mobile GUI Components by Deep Learning. arXiv preprint arXiv:2003.00380.

15. Chen, J., Chen, C., Xing, Z., Xia, X., Zhu, L., Grundy, J., & Wang, J. (2020). Wireframe-based UI design search through image autoencoder. ACM Transactions on Software Engineering and Methodology (TOSEM), 29(3), 1-31.

16. Islam, M. J., Pan, R., Nguyen, G., & Rajan, H. (2020). Repairing Deep Neural Networks: Fix Patterns and Challenges. arXiv preprint arXiv:2005.00972.

17. Gao, X., Saha, R. K., Prasad, M. R., & Roychoudhury, A. Fuzz Testing based Data Augmentation to Improve Robustness of Deep Neural Networks.

18. Li, K., Xiang, Z., Chen, T., Wang, S., & Tan, K. C. (2020). Understanding the Automated Parameter Optimization on Transfer Learning for CPDP: An Empirical Study. arXiv preprint arXiv:2002.03148.