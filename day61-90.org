* Duration
From 25 Aug 2020 to

* Content
1. [[#day-61-nlp][Day 61: Contextual Word Representations: A Contextual Introduction]]
2. [[#day-62-se][Day 62: Neurological Divide: An fMRI Study of Prose and CodeWriting]]
3. [[#day-63-se][Day 63: Is Static Analysis Able to Identify Unnecessary Source Code?]]
4. [[#day-64-nlp][Day 64: ALBERT: A Lite BERT for Self-supervised Learning of Language Representations]]
5. [[#day-65-se][Day 65: A Large Scale Study of Long-Time Contributor Prediction for GitHub Projects]]
6. [[#day-66-se][Day 66: On the Generalizability of Neural Program Analyzers with respect to Semantic-Preserving Program Transformations]]
7. [[#day-67-se][Day 67: Reformulating Queries for Duplicate Bug Report Detection]]
8. [[#day-68-se][Day 68: Automatic Duplicate Bug Report Detection using Information Retrieval-based versus Machine Learning-based Approaches]]
9. [[#day-69-se][Day 69: Train One Get One Free: Partially Supervised Neural Network for Bug Report Duplicate Detection and Clustering]]
10. [[#day-70-se][Day 70: An HMM-Based Approach for Automatic Detection and Classification of Duplicate Bug Reports]]
11. [[#day-71-se][Day 71: Towards Word Embeddings for Improved Duplicate Bug Report Retrieval in Software Repositories]]
12. [[#day-72-se][Day 72: Duplicate Bug Report Detection Using Dual-Channel Convolutional Neural Networks]]
13. [[#day-73-se][Day 73: LWE: LDA refined Word Embeddings for duplicate bug report detection | DWEN: Deep Word Embedding Network for Duplicate Bug Report Detection in Software Repositories]]
14. [[#day-74-se][Day 74: On Usefulness of the Deep-Learning-Based Bug Localization Models to Practitioners]]
15. [[#day-75-se][Day 75: On the Relationship between Bug Reports and Queries for Text Retrieval-based Bug Localization]]
16. [[#day-76-se][Day 76: A Large-Scale Comparative Evaluation of IR-Based Tools for Bug Localization]]
17. [[#day-77-se][Day 77: BuGL - A Cross-Language Dataset for Bug Localization]]
18. [[#day-78-se][Day 78: Bench4BL: Reproducibility Study on the Performance of IR-Based Bug Localization]]

* Day 61: NLP
- *Title*: Contextual Word Representations: A Contextual Introduction
- *Year*: 2020

The following content is referred from [1]
** Discrete words
a sequence of characters

each word type was given a unique (and more or less arbitrary) nonnegative inter value

*** Advantages
1. every word type was stored in the same amount of memory
2. array-based data structures could be used to index other information by word types

*** Disadvantages
the integers did not mean anything

The assignment might be arbitrary, alphabetical, or in the order word tokens

** Words vs Distributional Vectors: Context as Meaning
It's useful to first map each input word token to its vector, and then "feed" the word vectors into the neural network model, which performs a task like translation.

** Contextual word vectors
ELMo, which stands for "embeddings from language models" (Peters et al., 2018a), brought a powerful advance in the form ofword token vectors—i.e., vectors for words in context, or contextual word vectors—that are pretrained on large corpora.

A longstanding data-fitting problem in NLP is language modeling, which refers to predicting the next word given a sequence of “history” words (briefly alluded to in our filling-in- the-blank example in Section 3). Many of word (type) vector alogorithms already in use were based on a notion fixed-size context.

A full explanation of the differences in the learning algorithms, particularly the neural network architectures, is out of scope for this introduction, but it’s fair to say that the space of possible learners for contextual word vectors has not yet been fully explored;

* Day 62: SE
- *Title*：Neurological Divide: An fMRI Study of Prose and CodeWriting
- *Year*: 2020
- *Proc*: ICSE

The following content is referred from [2]
** Problem
Recent efforts have investigated the neural processes associated with reading and comprehending code — however, we lack a thorough understanding of the human cognitive processes underlying code writing.

** Method
They leverage functional brain imaging to investigate neural representations of code writing in comparison to prose writing.

They present the first human study in which participants wrote code and prose while undergoing a functional magnetic resonance imaging (fMRI) brain scan, making use of a full-sized fMRI-safe QWERTY keyboard.

** Result
They find that code writing and prose writing are significantly dissimilar neural tasks. While prose writing entails significant left hemisphere activity associated with language, code writing involves more activations of the right hemisphere, including regions associated with attention control, working memory, planning and spatial cognition. These findings are unlike existing work in which code and prose comprehension were studied. By contrast, we present
the first evidence suggesting that code and prose writing are quite dissimilar at the neural level.

** Future work
This unexpected result — that the production of code and prose rely on highly distinct cognitive substrates — though quite preliminary, paves the way forfuture investigations analogous to those based on medical imaging for prose writing. In addition to developing a foundational understanding of code writing, this empirical distinction may be leveraged to develop tools and pedagogies (e.g., transfer training), subsequently affecting large scale workforce retraining and educational reform. Moreover, neurological evidence that code and prose writing are not as intertwined as conventionally thought may encourage more diverse participation in computer science.

* Day 63: SE
- *Title*: Is Static Analysis Able to Identify Unnecessary Source Code?
- *Year*: 2020
- *Journal*: TOSEM

The following content is referred from [3]
** Problem
Grown software systems often contain code that is not necessary anymore. Such unnecessary code wastes resources during development and maintenance, for example, when preparing code for migration or certification. Running a profiler may reveal code that is not used in production, but it is often time-consuming to obtain representative data in this way.

** Method
We investigate to what extent a static analysis approach, which is based on code stability and code centrality, is able to identify unnecessary code and whether its recommendations are relevant in practice. To study the feasibility and usefulness of our approach, we conducted a study involving 14 open-source and closedsource software systems. As there is no perfect oracle for unnecessary code, we compared recommendations for unnecessary code with historical cleanups, runtime usage data, and feedback from 25 developers of five software projects

They implemented their approach as a recommender system to evaluate our work on 14 opensource and closed-source software systems.

** Result
The results suggest that static analysis can provide quick feedback on unnecessary code and is useful in practice.

** Future work
In thiswork, they focused on unnecessary code from a development andmaintenance perspective. It would be interesting to see whether similar approaches help test developers to focus their test effort on relevant parts of the software system.

* Day 64
- *Title*: Albert: A lite bert for self-supervised learning of language representations

- *Year*: 2019
- *Proc*: ICLR 2020

** Problem
Increasing model size when pretraining natural language representations often results in improved performance on downstream tasks. However, at some point further model increases become harder due to GPU/TPU memory limitations and longer training times.

** Method
To address these problems, they present two parameter-reduction techniques to lower memory consumption and increase the training speed of BERT.

ALBERT incorporates two parameter reduction techniques that lift the major obstacles in scaling pre-trained models.
- The first one is a factorized embedding parameterization.
- The second technique is cross-layer parameter sharing.

They also introduce a self-supervised loss for sentence-order prediction (SOP). SOP primary focuses on inter-sentence coherence and is designed to address the ineffectiveness.

** Result
Comprehensive empirical evidence shows that their proposed methods lead to models that scale much better compared to the original BERT.

** Future Work
An important next step is thus to speed up the training and inference speed of ALBERT through methods like sparse attention and block attention.

* Day 65
- *Title*: A Large Scale Study of Long-Time Contributor Prediction for GitHub Projects
- *Year*: 2020
- *Proc*: TSE

** Problem
The continuous contributions made by long time contributors (LTCs) are a key factor enabling open source software (OSS) projects to be successful and survival. We study GITHUB as it has a large number of OSS projects and millions of contributors, which enables the study of the transition from newcomers to LTCs. They investigate whether they can effectively predict newcomers in OSS projects to be LTCs based on their activity data that is collected from GITHUB.

** Method
They collect GITHUB data from GHTorrent, a mirror of GITHUB data. They select the most popular 917 projects, which contain 75,046 contributors. We determine a developer as a LTC of a project if the time interval between his/her ﬁrst and last commit in the project is larger than a certain time T. In the experiment, they use three different settings on the time interval: 1, 2, and 3 years. There are 9,238, 3,968, and 1,577 contributors who become LTCs of a project in three settings of time interval, respectively.

*** Evaluation metric
They use AUC, namely Area Under the receiver operating characteristic (ROC) Curve, to evaluate the effectiveness of the proposed prediction models. The ROC curve is created by plotting the true positive rate (TPR) against the false positive rate (FPR) across all thresholds.

** Result
To build a prediction model, they extract many features from the activities of developers on GITHUB, which group into ﬁve dimensions: developer proﬁle, repository proﬁle, developer monthly activity, repository monthly activity, and collaboration network. They apply several classiﬁers including naive Bayes, SVM, decision tree, kNN and random forest. They ﬁnd that random forest classiﬁer achieves the best performance with AUCs of more than 0.75 in all three settings of time interval for LTCs. They also investigate the most important features that differentiate newcomers who become LTCs from newcomers who stay in the projects for a short time.

** Future work
In the future, they want to collect more developers’ activities in OSS projects and further validate the effectiveness of our approach using more developers and projects.

* Day 66
- *Title*: On the Generalizability of Neural Program Analyzers with respect to Semantic-Preserving Program Transformations
- *Year*: 2020

The following content is referred from [66]

** Problem
With the prevalence of publicly available source code repositories to train deep neural network models, neural program analyzers can do well in source code analysis tasks such as predicting method names in given programs that cannot be easily done by traditional program analyzers. lthough such analyzers have been tested on various existing datasets, the extent in which they generalize to unforeseen source code is largely unknown.

** Method
They propose to evaluate the generalizability of neural program analyzers with respect to semantic-preserving transformations: a generalizable neural program analyzer should perform equally well on programs that are of the same semantics but of different lexical appearances and syntactical structures.

- 3 Java datasets
- 3 neural network models for code: code2vec, code2seq, Gated Graph Neural Networks (GGNN)

nine neural program analyzers for Evaluation

** Result
Their results show that even with small semantically preserving changes to the programs, these neural program analyzers often fail to generalize their performance. Their results also suggest that neural program analyzers based on data and control dependencies in programs generalize better than neural program analyzers based only on abstract syntax trees. On the positive side, they observe that as the size of training dataset grows and diversifies the generalizability of correct predictions produced by the analyzers can be improved too.

** Future work
Future work that includes more semantic-preserving and even some semi-semantic-preserving transformations in the approach and adapts more fine-grained predication change metrics may further extend the applicability of their approach to various neural program analyzers designed for different tasks.

* Day 67: SE
- *Title*: Reformulating Queries for Duplicate Bug Report Detection
- *Year*: 2019
- *Proc*: SANER

The following content is referred from [7]
** Problem
When the number of bug reports is large, finding duplicates can be a time-consuming and error-prone activity.

** Method
The tools use the new bug report as a query and then the user inspects the ranked list of retrieved bug reports to check if any are duplicates of the new bug report. At some point, if a duplicate is not found, the user chooses to stop and mark the bug as new or tries some other approach.


The paper rethinks tool-supported duplicate bug report detection as a two-step process, using the entire new bug report as a query in the first step, for retrieving N bug reports, and a reformulated query in the second step, for retrieving additional N bug reports.


most bug reports have an inherent structure, consisting of the bug title (BT), the observed behavior (OB), the expected behavior (EB), and the steps to reproduce the noted bug (S2R)

They evaluated the three reformulation strategies using a duplicate bug report retrieval approach based on Lucene.


They argue that duplicate bug report detection approaches, based on text retrieval, should be viewed as a two-step process.

** Result
They found that using the observed behavior description, together with the title, leads to the best retrieval performance. Using only the title or only the observed behavior for reformulation is also better than retrieval with the initial query. The reformulation strategies lead to 56.6%-78% average retrieval improvement, over using the initial query only.

** Future work
focus on automatically reducing queries based on specific bug descriptions

* Day 68: SE
- *Title*: Automatic Duplicate Bug Report Detection using Information Retrieval-based versus Machine Learning-based Approaches
- *Year*: 2020

The following content is referred from [68]

** Problem
There are two main approaches for automatic DBRD, including information retrieval (IR)-based and machine learning (ML)-based.

** Method
The Android dataset is used for evaluation, and about 2 million pairs of bug reports are analyzed for 59 bug reports, which were duplicate.

** Result
The results show that the MLbased approach has better validation performance, incredibly about 40%. Besides, the ML-based approach has a more reliable criterion for evaluation like accuracy, precision, and recall versus an IR-based approach, which has just mean average precision (MAP) or rank metrics.

** Future work
There is a question for future works which how can reduce the number of comparisons to find the duplicates as soon as possible; otherwise, we have to check all bug reports (like IR-based approach) to find out a new bug report is duplicate or not.

* Day 69: SE
- *Title*: 
- *Year*: 2019

The following content is referred from [69]
** Problem
Tracking user reported bugs requires considerable engineering effort in going through many repetitive reports and assigning them to the correct teams.

(1) detect if two bug reports are duplicates, and (2) aggregate them into latent topics.

** Method
Leveraging the assumption that learning the topic of a bug is a sub-task for detecting duplicates, we design a loss function that can jointly perform both tasks but needs supervision for only duplicate classiﬁcation, achieving topic clustering in an unsupervised fashion. They use a two-step attention module that uses self-attention for topic clustering and conditional attention for duplicate detection. They study the characteristics of two types of real world datasets that have been marked for duplicate bugs by engineers and by nontechnical annotators.

** Result
The results demonstrate that our model not only can outperform stateof-the-art methods for duplicate classiﬁcation on both cases, but can also learn meaningful latent clusters without additional supervision.

** Future work
1. the challenges of annotating a user reported bug dataset with non-technical annotators, as opposed to using annotations from engineers
2. the same underlying problem from the engineering side


* Day 70: SE
- *Title*: An HMM-Based Approach for Automatic Detection and Classification of Duplicate Bug Reports
- *Year*: 2019

The following content is referred from [70]
** Problem
Software projects rely on their issue tracking systems to guide maintenance activities of software developers. Bug reports submitted to the issue tracking systems carry crucial information about the nature of the crash (such as texts from users or developers and execution information about the running functions before the occurrence of a crash). Typically, big software projects receive thousands of reports every day.

automatically detect duplicate bug reports

** Method
using execution traces and Hidden Markov Models

They provide a better evaluation of our approach using MAP and Recall@rank-k by varying k from 1 to 20.

1. extract BRs with stack traces from bug reports repositories of Firefox and GNOME, which use Bugzilla for BR tracking
2. Once they have the BRs, they search for the duplicates one by examining the BR status. They create duplicate BR groups (DG) where each group DG contains stack traces of one master BR and those of all its duplicates.

They train HMM using 60% of the traces, validate the HMM using 10% of the traces, and test the model using 30% of the traces of this DG and every other DG.

*** Related work
- Textual-based approaches
developers and users submit information related to the crash in the summary and textual description part of a bug report


Information retrieval (IR) techniques are widely used to calculate the similarity scores between queries and the retrieved data.


- Execution information-based approaches

** Result
a list of rank-1 bug reports, recall values of 80% and 63% have been achieved on Firefox and GNOME datasets, respectively. With the same list of bug reports, our approach detects the duplication of a given report with an average MAP value of 87% and 71.5% on Firefox and GNOME datasets, respectively.

** Future work
- investigate more BRs from additional software systems
- improve the effectiveness of our proposed approach in terms of recall and MAP scores
- extend the dataset by considering more threads
- study how to combine stack traces with other BR fields such as BR descriptions and comments
- A combined approach should not treat stack traces as documents, as it is done in the literature, but model the temporal order of sequences of function calls, just as it is done in this paper

* Day 71: SE
- *Title*: Towards Word Embeddings for Improved Duplicate Bug Report Retrieval in Software Repositories
- *Year*: 2018
- *Proc*: International Conference on the Theory of Information Retrieval

The following content is referred from [11]
** Problem
retrieval of top-k similar bug reports for a new bug report by employing word embeddings

** Method
1. consider each bug report as a text document and use it for training word embedding models
2. using the trained word embedding model, they convert bug reports into vectors and retrieve the top-k most similar bug reports

*** Datasets
Mozilla and Open Office

*** Details
Only use two textual components from each bug report: title (or summary), and description. Both title and description are combined and a word embedding model of demension D is learnt


After learning the model, all the bug reports present in the database are transformed into vectors by averaging the individual vectors of the words in the bug reports

In order to find duplicates of a new bug report, the most similar k bug reports are retrieved by means of cosine similarity between a new report and the existing bug reports.

The major steps:
1. Tokenization
2. Training
3. Vectorization of Bug Reports
4. Compute Distances

** Result
*** Approaches
word embedding models:
1. Skipgram
2. CBOW
3. Fast-Skipgram
4. Fast-CBOW
5. DBOW
6. PV-DM

baseline approaches:
1. BM25F
2. LDA
3. An approach from previous work

*** Performance Metrics
Recall rate: it measures the accuracy of the duplicate retrieval system in terms of counting the percentage of duplicates (a query which is a duplicate) for which the master bug-report is found within the top-K search results

AUC-ROC (Area Under Curve of Receiver Operating Characteristic)

It can be seen that the proposed approach with DBOW model performs the best compared to the BM25F and LDA baselines and the approach proposed in previous work.

** Future work
investigate other available word embedding approaches and induce supervision during training using tagged duplicate bug reports

* Day 72: SE
- *Title*: Duplicate Bug Report Detection Using Dual-Channel Convolutional Neural Networks
- *Year*: 2020
- *Proc*: ICPC

The following content is referred from [12]
** Problem
duplicate bug report detection

** Method
They propose a duplicate bug report detection approach based on Dual-Channel Convolutional Neural Networks (DC-CNN). They present a novel bug report pair representation, i.e., dual-channel matrix through concatenating two single-channel matrices representing bug reports. Such bug report pairs are fed to a CNN model to capture the correlated semantic relationships between bug reports. Then, their approach uses the association features to classify whether a pair of bug reports are duplicate or not.

** Result
They evaluate their approach on three large datasets from three open-source projects, including Open Office, Eclipse, Net Beans and a larger combined dataset, and the accuracy of classification reaches 0.9429, 0.9685, 0.9534, 0.9552 respectively. Such performance outperforms the two state-of-the-art approaches which also use deep-learning techniques. The results indicate that their dual-channel matrix representation is effective for duplicate bug report detection.

** Future work
In the future, one can investigate how to make use of more structured information to improve their approach. Additionally, more empirical studies can be performed to validate our approach on both open source and industrial projects.

* Day 73: SE
- *Title*: LWE: LDA refined Word Embeddings for duplicate bug report detection
- *Year*: 2018
- *Proc*: ICSE

The following content is referred from [13]
** Problem
Detecting duplicate bug reports is an important task in order to avoid the assignment of a same bug to different developers.

** Method
combine LDA and word embeddings to leverage the strengths of both approaches for the task

While word embeddings have a high precision (i.e. two reports which are reported as similar will have very high chances of being similar), LDA has a high recall (i.e. two reports which are reported as non-similar will have very high chances of being non-similar).

They proposed a 'LDA refined Word Embeddings' (LWE)

*** Steps
1. They use an LDA model to extract top-n most similar bug reports, thereby pruning the rest.
2. From the extracted top-n reports, they extract the final top-k most similar reports (k < n) by means of a word embedding model which are shown to the Triager.
** Result
*** Performance metric
recall rate measures the accuracy of the duplicate detection system in terms of counting the percentage of duplicates (a query which is a duplicate) for which the master bug-report is found within the top-k search results


This indicates that the Skipgram models gives high probability for duplicate reports (i.e. high precision) and LDA is giving low probability for non-duplicate reports (i.e. high recall)
** Future work
They plan on carrying out an in-depth investigation on why LDA has a high recall and word embeddings have high precision for this task. They plan on building a model through which we can train both LDA and word embeddings together.

They plan on investigating if we can use these signals to improve the results such as using supervised LDA and/or supervised word embeddings.


- *Title*: DWEN: Deep Word Embedding Network for Duplicate Bug Report Detection in Software Repositories
- *Year*: 2018
- *Proc*: ICSE

The following content is referred from [13]
** Problem
Capturing and tagging duplicate bug reports is scurcial in order to avoid assignment of the same bug to different developers. Efforts have been made in the past to detect duplicate bug reports by using topic modelling, discriminative methods, meta-attributes.

** Method
they train a deep neural network on top of bug reports vectors created from a word embedding model

1. Word embedding training
2. Transformantion into Document Vectors of size
3. Training of the deep neural network

** Result
They compare BM25F as an information retrieval baseline, LDA as a topic modelling baseline.

** Future work
They aim at investigating different word embedding models for DWEN. They also aim to reduce the training to a single step compared the two step training process of training embeddings and deep neuralnetwork.

* day 74: SE
- *Title*: On Usefulness of the Deep-Learning-Based Bug Localization Models to Practitioners
- *Year*: 2019

The following content is referred from [74]
** Problem
The practitioners, on the other hand, expect a bug localization tool to meet certain criteria, such as trustworthiness, scalability, and efficiency.

They would like to investigate whether deep learning models meet the expectations of practitioners or not.

** Method
They constructed a Convolution Neural Network and a Simple Logistic model to examine their effectiveness in localizing bugs. They train these models on ﬁve open source projects written in Java and compare their performance with the performance of other state-of-the-art models trained on these datasets.

** Result
The experiments show that although the deep learning models perform better than classic machine learning models, they meet the adoption criteria set by the practitioners only partially.

** Future work
It also highlights the need for standardization of performance benchmarks to ensure that bug localization models are assessed equitably and realistically.

* day 75: SE
- *Title*: On the Relationship between Bug Reports and Queries for Text Retrieval-based Bug Localization
- *Year*: 2020
- *Jounral*: EMSE

The following content is referred from [75]
** Problem
Bug localization is the process by which a developer identifies buggy code that needs to be fixed to make a system safer and more reliable.

** Method
Text retrieval (TR): a software developer trying to localize the bug formulates a natural language query describing the observed bug. The query is then run through a TR engine, which returns a ranked list of code components (e.g., classes or methods, depending on the desired granularity), containing the most relevant results in the top-most positions.

This is an empirical study providing new evidence on the true potential of TR bug localization approaches and the significant impact that optimizing queries can have on their effectiveness.


used 803 bug repost from 15 open source systems used in previous bug localization experiments

** Result
highly performing queries can be extracted from the bug report text, in order to make TR eﬀective even without the aforementioned positive biases

given a bug report, they can often obtain an optimal query using only words selected from its vocabulary, even when localization hints are not present

** Future work
1. The ﬁrst is to extend the analysis to a larger set of bugs, particularly from more modern systems. Additionally, we should consider method-level golden sets.
2. The second research direction is to find a way of generating sufficient training datato attempt the construction of automatic models for formulating near-optimalqueries. The most intuitive way to do so is to log query metrics for each stepin the evolution of a query from an initial vocabulary to a near-optimal query.

* day 76: SE
- *Title*: A Large-Scale Comparative Evaluation of IR-Based Tools for Bug Localization
- *Year*: 2020
- *Proc*: MSR

The following content is referred from [16]

This paper is an empirical study on a large-scale comparative evaluation of IR-based tools for automatic bug localization.

They divide the tools into three generations:
1. The first-generation tools: purely on the Bag-of-Words (BoW) modeling of software libraries
2. The second-generation tools: augment BoW-based modeling with two additional pieces of information: historical data, and structured information
3. The third-generation tools: exploit proximity, order, and semantic relationships between the terms

Retrieving relevant source code files from software libraries in response to a bug report query plays an important role in the maintenance of a software project.

** Design
over 20,000 bug reports drawn from a diverse collection of Java, C/C++, and Python projects

*** Evaluation Metrics
Mean Average Precision (MAP), this metric is the mean of the Average Precisions (AP) calculated for each of the bug report queries.

The MAP values are subject to statistical significance testing using the Student's Paired t-Test.

** Result
The third-generation tools are significantly superior to the older tools. The word embeddings generated using code files writted in one language are effective for retrieval from code libraries in other languages.

** Future work
For future they intend to evaluate more retrieval algorithms from each generation on open-source as well industry projects.

* Day 77: SE
- *Title*: BuGL - A Cross-Language Dataset for Bug Localization
- *Year*: 2020

The following content is referred from [17]
** Problem
The existing dataset do not comprise projects of other programming languages, despite of the need to investigate specific and cross project bug localization.

** Method
They present BuGL, a large-scale cross-language dataset. BuGL constitutes of more than 10,000 bug reports drawn from opensource projects written in four programming languages, namely C, C++, Java, and Python. The dataset consists of information which includes Bug Reports and Pull-Requests.

** Future work
- To extend this dataset, they plan to include repositories from various domains and programming languages. The aim is to include a diverse range of bugs that can help to formulate new sets of bug localization techniques.

- More emphasis will be given towards adding new features in the dataset for more in-depth analysis of bug reports.

- They are also planning to create an automatic tool that could deal with duplicate bug reports and pull requests.

* day 78: SE
- *Title*: Bench4BL: Reproducibility Study on the Performance of IR-Based Bug Localization
- *Year*: 2018
- *Proc*: ISSTA

The following content is referred from [18]
** Problem
information retrieval (IR) techniques to automate the localization of buggy files

They report on a comprehensive reproduction study of six state-of-the-art IR-based bug localization (IRBL) techniques. This study applies not only subjects used in existing studies (old subjects) but also 46 new subjects (61,431 Java files and 9,459 bug reports) to the IRBL techniques.

** Method
*** Performance Metrics
1. Precision
Precision@k, this metric presents an estimation of how many files are correctly recommended within given top k files
2. Recall
Recall@k, this metric estimates how many files are correctly recommended within given top k files over the actually fixed files by a developer for a given bug report
3. Average Precision (AP)
4. Mean Average Precision (MAP)
5. Mean Reciprocal Rank (MRR)
*** IRBL Techniques
- (2012) - BugLocator [53] leverages similar bug reports that have been previously fixed and relies on revised Vector Space Model (rVSM) for the recommendation. 
- (2013) - BLUiR [37] extracts code entities such as classes, methods, and variable names from bug reports and leverages them to localize files. 
- (2014) - BRTracer [46] analyzes stack traces shown in bug reports to improve bug localization accuracy. 
- (2014) - AmaLgam [44] utilizes revision history in addition to similar reports and code entities. 
- (2015) - BLIA [52] combines information such as similar reports, revision history, code entities, and stack trace information all together to improve the performance of IRBL. 
- (2016) - Locus [45], the most recent technique, leverages code change information.

** Result
1. IRBL techniques generally yield better performance on recent subjects. To estimate the actual performance that is reached by state-of-the-art approaches, they recommend that researchers should use up-to-date subjects.

2. Their experiments have shown that when matching the bug report with its code version, IRBL techniques are most effective. IRBL techniques should consider exploiting version metadata from bug report and select appropriate code base for attempting to localize the bug.

** Future work
(1) Investigating relationships between project/report/file characteristics and the performance of different IRBL techniques (cf. D&C approach [19]), (2) building a decision model that predicts which IRBL technique performs better than others for a given project of file, and (3) improving preprocessing steps of IRBL techniques to reduce noise.

* Reference
1. Smith, N. A. (2019). Contextual word representations: A contextual introduction. arXiv preprint arXiv:1902.06006.

2. Krueger, R., Huang, Y., Liu, X., Santander, T., Weimer, W., & Leach, K. (2020). Neurological Divide: An fMRI Study of Prose and Code Writing. In 2020 IEEE/ACM 42nd International Conference on Software Engineering (ICSE) (Vol. 13).

3. HAAS, R., NIEDERMAYR, R., ROEHM, T., & APEL, S. (2019). Is Static Analysis Able to Identify Unnecessary Source Code?. Transactions on Software Engineering and Methodology (TOSEM), 178.

4. Lan, Z., Chen, M., Goodman, S., Gimpel, K., Sharma, P., & Soricut, R. (2019). Albert: A lite bert for self-supervised learning of language representations. arXiv preprint arXiv:1909.11942.

5. Bao, L., Xia, X., Lo, D., & Murphy, G. C. (2019). A large scale study of long-time contributor prediction for GitHub projects. IEEE Transactions on Software Engineering.

6. Rabin, M., Islam, R., Bui, N. D., Yu, Y., Jiang, L., & Alipour, M. A. (2020). On the Generalizability of Neural Program Analyzers with respect to Semantic-Preserving Program Transformations. arXiv preprint arXiv:2008.01566.

7. Chaparro, O., Florez, J. M., Singh, U., & Marcus, A. (2019, February). Reformulating queries for duplicate bug report detection. In 2019 IEEE 26th International Conference on Software Analysis, Evolution and Reengineering (SANER) (pp. 218-229). IEEE.

8. Neysiani, B. S., & Babamir, S. M. (2020). Automatic Duplicate Bug Report Detection using Information Retrieval-based versus Machine Learning-based Approaches. In IEEE 6th International Conference on Web Research (ICWR).

9. Poddar, L., Neves, L., Brendel, W., Marujo, L., Tulyakov, S., & Karuturi, P. (2019). Train one get one free: Partially supervised neural network for bug report duplicate detection and clustering. arXiv preprint arXiv:1903.12431.

10. Ebrahimi, N., Trabelsi, A., Islam, M. S., Hamou-Lhadj, A., & Khanmohammadi, K. (2019). An HMM-based approach for automatic detection and classification of duplicate bug reports. Information and Software Technology, 113, 98-109.

11. Budhiraja, A., Dutta, K., Shrivastava, M., & Reddy, R. (2018, September). Towards word embeddings for improved duplicate bug report retrieval in software repositories. In Proceedings of the 2018 ACM SIGIR International Conference on Theory of Information Retrieval (pp. 167-170).

12. He, J., Xu, L., Yan, M., Xia, X., & Lei, Y. Duplicate Bug Report Detection Using Dual-Channel Convolutional Neural Networks.

13. Budhiraja, A., Reddy, R., & Shrivastava, M. (2018, May). Lwe: Lda refined word embeddings for duplicate bug report detection. In Proceedings of the 40th International Conference on Software Engineering: Companion Proceeedings (pp. 165-166). | Budhiraja, A., Dutta, K., Reddy, R., & Shrivastava, M. (2018, May). DWEN: deep word embedding network for duplicate bug report detection in software repositories. In Proceedings of the 40th International Conference on Software Engineering: Companion Proceeedings (pp. 193-194).

14. Polisetty, S., Miranskyy, A., & Başar, A. (2019, September). On Usefulness of the Deep-Learning-Based Bug Localization Models to Practitioners. In Proceedings of the Fifteenth International Conference on Predictive Models and Data Analytics in Software Engineering (pp. 16-25).

15. Mills, C., Parra, E., Pantiuchina, J., Bavota, G., & Haiduc, S. (2020). On the relationship between bug reports and queries for text retrieval-based bug localization. Empirical Software Engineering, 1-42.

16. AKBAR, S., & Kak, A. C. (2020, October). A Large-Scale Comparative Evaluation of IR-Based Tools for Bug Localization. In Conference on Mining Software Repositories (MSR’20).

17. Muvva, S., Rao, A. E., & Chimalakonda, S. (2020). BuGL--A Cross-Language Dataset for Bug Localization. arXiv preprint arXiv:2004.08846.

18. Lee, J., Kim, D., Bissyandé, T. F., Jung, W., & Le Traon, Y. (2018, July). Bench4bl: reproducibility study on the performance of ir-based bug localization. In Proceedings of the 27th ACM SIGSOFT international symposium on software testing and analysis (pp. 61-72).