* Duration
From 25 Aug 2020 to

* Content
1. [[#day-61-nlp][Day 61: Contextual Word Representations: A Contextual Introduction]]
2. [[#day-62-se][Day 62: Neurological Divide: An fMRI Study of Prose and CodeWriting]]
3. [[#day-63-se][Day 63: Is Static Analysis Able to Identify Unnecessary Source Code?]]
4. [[#day-64-nlp][Day 64: ALBERT: A Lite BERT for Self-supervised Learning of Language Representations]]

* Day 61: NLP
- *Title*: Contextual Word Representations: A Contextual Introduction
- *Year*: 2020
- *Proc*: 

The following content is referred from [1]

** Discrete words
a sequence of characters

each word type was given a unique (and more or less arbitrary) nonnegative inter value

*** Advantages
1. every word type was stored in the same amount of memory
2. array-based data structures could be used to index other information by word types

*** Disadvantages
the integers did not mean anything

The assignment might be arbitrary, alphabetical, or in the order word tokens

** Words vs Distributional Vectors: Context as Meaning
It's useful to first map each input word token to its vector, and then "feed" the word vectors into the neural network model, which performs a task like translation.

** Contextual word vectors
ELMo, which stands for "embeddings from language models" (Peters et al., 2018a), brought a powerful advance in the form ofword token vectors—i.e., vectors for words in context, or contextual word vectors—that are pretrained on large corpora.

A longstanding data-fitting problem in NLP is language modeling, which refers to predicting the next word given a sequence of “history” words (briefly alluded to in our filling-in- the-blank example in Section 3). Many of word (type) vector alogorithms already in use were based on a notion fixed-size context.

A full explanation of the differences in the learning algorithms, particularly the neural network architectures, is out of scope for this introduction, but it’s fair to say that the space of possible learners for contextual word vectors has not yet been fully explored;

* Day 62: SE
- *Title*：Neurological Divide: An fMRI Study of Prose and CodeWriting
- *Year*: 2020
- *Proc*: ICSE

The following content is referred from [2]

** Problem
Recent efforts have investigated the neural processes associated with reading and comprehending code — however, we lack a thorough understanding of the human cognitive processes underlying code writing.

** Method
They leverage functional brain imaging to investigate neural representations of code writing in comparison to prose writing.

They present the first human study in which participants wrote code and prose while undergoing a functional magnetic resonance imaging (fMRI) brain scan, making use of a full-sized fMRI-safe QWERTY keyboard.

** Result
They find that code writing and prose writing are significantly dissimilar neural tasks. While prose writing entails significant left hemisphere activity associated with language, code writing involves more activations of the right hemisphere, including regions associated with attention control, working memory, planning and spatial cognition. These findings are unlike existing work in which code and prose comprehension were studied. By contrast, we present
the first evidence suggesting that code and prose writing are quite dissimilar at the neural level.

** Future work
This unexpected result — that the production of code and prose rely on highly distinct cognitive substrates — though quite preliminary, paves the way forfuture investigations analogous to those based on medical imaging for prose writing. In addition to developing a foundational understanding of code writing, this empirical distinction may be leveraged to develop tools and pedagogies (e.g., transfer training), subsequently affecting large scale workforce retraining and educational reform. Moreover, neurological evidence that code and prose writing are not as intertwined as conventionally thought may encourage more diverse participation in computer science.

* Day 63: SE
- *Title*: Is Static Analysis Able to Identify Unnecessary Source Code?
- *Year*: 2020
- *Journal*: TOSEM

The following content is referred from [3]
** Problem
Grown software systems often contain code that is not necessary anymore. Such unnecessary code wastes resources during development and maintenance, for example, when preparing code for migration or certification. Running a profiler may reveal code that is not used in production, but it is often time-consuming to obtain representative data in this way.

** Method
We investigate to what extent a static analysis approach, which is based on code stability and code centrality, is able to identify unnecessary code and whether its recommendations are relevant in practice. To study the feasibility and usefulness of our approach, we conducted a study involving 14 open-source and closedsource software systems. As there is no perfect oracle for unnecessary code, we compared recommendations for unnecessary code with historical cleanups, runtime usage data, and feedback from 25 developers of five software projects

They implemented their approach as a recommender system to evaluate our work on 14 opensource and closed-source software systems.

** Result
The results suggest that static analysis can provide quick feedback on unnecessary code and is useful in practice.

** Future work
In thiswork, they focused on unnecessary code from a development andmaintenance perspective. It would be interesting to see whether similar approaches help test developers to focus their test effort on relevant parts of the software system.

* day 63
- *Title*: ALBERT: A LITE BERT FOR SELF-SUPERVISED LEARNING OF LANGUAGE REPRESENTATIONS

- *Year*: 2019
- *Proc*: ICLR 2020
** Problem
Increasing model size when pretraining natural language representations often results in improved performance on downstream tasks. However, at some point further model increases become harder due to GPU/TPU memory limitations and longer training times.

** Method
To address these problems, they present two parameter-reduction techniques to lower memory consumption and increase the training speed of BERT.

ALBERT incorporates two parameter reduction techniques that lift the major obstacles in scaling pre-trained models.
- The first one is a factorized embedding parameterization.
- The second technique is cross-layer parameter sharing.

They also introduce a self-supervised loss for sentence-order prediction (SOP). SOP primary focuses on inter-sentence coherence and is designed to address the ineffectiveness.

** Result
Comprehensive empirical evidence shows that their proposed methods lead to models that scale much better compared to the original BERT.

** Future Work
An important next step is thus to speed up the training and inference speed of ALBERT through methods like sparse attention and block attention.

* Reference
1. Smith, N. A. (2019). Contextual word representations: A contextual introduction. arXiv preprint arXiv:1902.06006.

2. Krueger, R., Huang, Y., Liu, X., Santander, T., Weimer, W., & Leach, K. (2020). Neurological Divide: An fMRI Study of Prose and Code Writing. In 2020 IEEE/ACM 42nd International Conference on Software Engineering (ICSE) (Vol. 13).

3. HAAS, R., NIEDERMAYR, R., ROEHM, T., & APEL, S. (2019). Is Static Analysis Able to Identify Unnecessary Source Code?. Transactions on Software Engineering and Methodology (TOSEM), 178.