#+TITLE: 365-day-paper-reading-challenge
#+AUTHOR: happygirlzt
#+DATETIME: 2020-06-26 Fri

* To readers
You could fork this repo and start your own 365-day challenge on your branch. Let's study together!

* Schedule
I will read an article related to deep learning, reinforcement learning, natural language processing, and/or software engineering every day. This paper reading is more about a breadth reading. I do not want all my reading is limited to my research project. I hope I could keep pace with the latest research in AI/ML/DL/SE. So, in that case, the paper reading will be in *coarse-grained* manner.

I will try to summarize the key idea of the paper. Currently, I want my daily notes to follow the structure:
1. Problem
2. Method
3. Result
4. Future work

I hope I could keep it work up and improve my ability to read and write.
* Contents
1. [[#day-0-se][Day 0]]
2. [[#day-1-se][Day 1]]
3. [[#day-2-se][Day 2]]
4. [[#day-3-nlp][Day 3]]
5. [[#day-4-se][Day 4]]
6. [[#day-5-se][Day 5]]
7. [[#day-6-se][Day 6]]
8. [[#day-7-se][Day 7]]
9. [[#day-8-se][Day 8]]
10. [[#day-9-se][Day 9]]
11. [[#day-10-se][Day 10]]
* Record
** Day 0: SE
- *Title*: How Bad Can a Bug Get? An Empirical Analysis of Software Failures in the OpenStack Cloud Computing Platform
- Tags: bug analysis, fault injection, OpenStack
- Proc.: ESEC/FSE 2019
The following content is referred from [1]
- Roadmap: Cloud management systems (CMS) are important and usually have residual software bugs. -> Many high-severity failures have been occurring in cloud infrastructures and caused bad results. -> Software developers invest effort in mitigating the consequences of residual bugs. -> They aim to empirically analyze the impact of high-severity failures in the context of a large-scale, industry-applied case study, to pave the way for failure mitigation strategies in CMS. -> OpenStack is widely used and is representative. -> They adopt *software fault injection* to accelerate the occurrence of failures caused by software bugs.
*** Problem
This paper investigated the impact of failures in the context widespread OpenStack CMS. Mainly related to *analyssi of bugs and failures* of cloud systems and *fault injection* in cloud systems.
*** Method
Performing *fault injection* and analyzing the impact of the resulting failures in terms of fail-stop behavior, failure detection through logging, and failure propagation across components.

+ They based fault injection on info on software bugs reported by OpenStack developers and users => characterize frequent bug patterns occurring in the project.

+ They performed a large fault injection campaign on the three subsystems of OpenStack (i.e., Nova, Cinder, and Neutron), a total of 911 experiments.

**** Bug analysis
They went through the problem reports and inspected the related source code. They figured out five most frequent bug types: (1) Wrong parameters value (2) Missing parameters (3) Missing function call (4) Wrong return value (5) Missing exception handlers.
**** Fault injection
They developed a tool to automate this task. The tool uses /ast/ module to generate an /abstract syntax tree (AST)/ representation of the source code. Then, it scans the AST to replace the node with a bug. Finally, it rewrites the modified AST into Python code, using the /astunparse/ Python module.

**** Failure data collection
Use workload generator to record the outcomes of both the API calls and of the assertion checks. They also collect all the log files generated by the CMS.

**** Failure analysis
Two perspectives.
- The first perspective
1. API error
2. Assertion failure
3. Assertion failure(s), followed by an API Error
4. No failure
- The second perspective
1. Failure in the faulty round only
2. Failure in the fault-free round (despite the faulty round)

*** Result
1. In the majority of the experiments, OpenStack failures were not mitigated by a fail-stop behavior. 31.3% of the failures was never notified to the user through exceptions. The others were only notified after a long delay.
2. In 8.5% of the experiments, there was no indication of the failure in the logs. -> high rish for system operators
3. 37.5% of the failures, the injected bugs propagated across several OpenStack components. 68.3% of these failures were notified by a different component from the injected one.
In summary, there is a risk that failures are not timely detected and notified, and they can sliently propagate through the system.
- *Dataset*
1. bug reports from OpenStack: highest severe bugs and have been fixed. ('Critical', 'Fix Committed' or 'Fix Released')
*** Future work
1. Deeper run-time verification of virtual resources
2. Increasing the logging coverage.
Current logging mechanisms in OpenStack only reports high-severity error messages for many of the failures. Some failures with late or no API errors that would benefit from logs to diagnose the failure, but they are missing.
3. Preventing corruptions of persistent data and shared state.

** Day 1: SE
- *Title*: Towards understanding bugs in an open source cloud management stack: An empirical study of OpenStack software bugs.


The following content is referred from [2]
- *Keywords*: Cloud management stack, Bug reports, OpenStack
*** Problem
Conduct an in-depth study to help developers to detect and fix the bugs in CMS.

An empirical study of several key characteristics of bugs in OpenStack.

*** Method
Their study computes general statistics of *50k* OpenStack bugs, including the evolution of bugs, the distribution of bugs, and the duration of bugs. Then, they selected 579 bugs for an in-depth study. They study the input factors for triggering the bugs, the consequences of the bugs, and how the bugs are fixed.

**** Data collection
They collected bug reports from Launchpad version control system of OpenStack.

There are 6 important levels, e.g., undecided, critical, high, medium, low and wishlist in OpenStack bugs. They focus on Critial, High and Medium bugs that are often more interesting to developers. In addition, they focus on /complete/ and /fixed/ bugs.
**** Basic text parsing
code removal, stop-word removal, stemming and lemmatization
**** Discourse pattern matching
Discourse patterns are rules that caputure the syntax and semantics of the text summarize a family of discourse patterns.

*** Bug triggering factors
From two perspectives: input factors and timing factors
*** Bug consequences
They study the scope of the impact and categorize the symptoms of the failures
*** Bug fixing
They categorize the bug fixes into four types: code fix, configuration fix, environment fix, and test case fix.
*** Contribution
1. The first comprehensive study on OpenStack bugs.
2. The first large-scale CMS-bug benchmark.

*** Result
1. The result indicate a large portion of bugs are related to incorrect configurations.
2. They report the consequences of the manifested bugs and incorrect output is the dominant majority (66.14%).
3. Most of the code fixing involive a small number of lines.

*** Future work
Further research on testing and diagnosis for cloud management stack bugs. It would be promising to investigate techniques that can
direct bug fixing based on the focused code locations, such as using machine learning classifiers to predict error-prone code regions and leveraging fault localization to pinpoint the faults.

** Day 2: SE
- *Title*: Is deep learning better than traditional approaches in tag recommendation for software information sites?

- *Journal*: Information and Software Technology
- *Keywords*: Deep learning, Data analysis, Tag recommendation

The following content is referred from [3]
*** Problem
Whether deep learning is better than traditional approaches in tag recommendation task for software information sites.
**** Formulation
They assume that any software object contains a description and a set of tags. These tags in a software information site and the tags associated with an object is a subset of these tags.

Given a large set of existing software objects that are attached with tags, how to automatically recommend a set of appropriate tags for a new software object.

This is a multi-label classification task.
*** Method
- 4 DL methods: TagCNN, TagRNN, TagHAN (Hierarchical Attention Networks) and TagRCNN (Recurrent Convolutional Neural Networks)
- 3 advanced traditional methods: EnTagRec, TagMulRec, and FastTagRec

A ten-round validation
**** Dataset
One large-scale software information site StackOverflow , 3 medium-scale software information sites Askubuntu, Serverfault, Unix and 6 small-scale sites Codereview, Freecode, Database Administrator, Wordpress, AskDifferent and Software Engineering
**** Evalution metrics
top-k prediction recall, the top-k prediction precision, and the top-k prediction f1-score

*** Result
The performance of TagRNN and TagHAN approaches are worse than traditional approaches in tag recommendation tasks. The performance of TagCNN and TagRCNN approaches are better than traditional approaches in tag recommendation tasks.
*** Future work
How to best represent the software object or artifact with a high quality vector is still a major challenge.

** Day 3: NLP

- *Title*: CodeBERT: A Pre-Trained Model for Programming and Natural Languages

The following context is referred from [4]
*** Problem
CodeBERT is a /bimodal/ pre-trained model for natural language (NL) and programming lan- guage (PL).

modality (n. 形式,形态,特征)

/bimodal/ datapoint is an individual function with paired documentation, and each /unimodal/ code is a function without paired documentation

*** Method
CodeBERT captures the semantic connection between natural language and programming language, and produces general-purpose representations that can broadly support NL-PL understanding tasks (e.g. natural language code search) and generation tasks (e.g. code documentation generation)

CodeBERT is trained on Github code repositories in 6 programming languages.

CodeBERT has exactly the same model architecture as RoBERTa-base.

They regard a piece of code as a sequence of tokens. (WordPiece)
*** Result
CodeBERT achieves state-of-the-art performance on both *natural language code search* and *code documentation generation*

CodeBERT is the first large NL-PL pre-trained model.
*** Future work
A potential direction to improve CodeBERT by incorporating AST.
1. One could learn better generators with bimodal evidence or more complicated neural architecture to improve the replaced token detection objective.
2. The CodeBERT itself could be further improved by generation-related learning objectives.
3. We can apply CodeBERT to more NL-PL related tasks, and extend it to more programming languages

** Day 4: SE
- *Title*: Examining the Impact of Self-admitted Technical Debt on Software Quality
- *Venue*: SANER-2016
The following context is referred from [5]

*** Problem
Understand the impact of self-admitted technical debt on software quality

*** Method
They focus on self-admitted technical debt that refers to errors that might be introduced due to intentional quick or temporary fixed.

They empirically investigate the relation between SATD and software quality in five open-source projects. They examine whether (i) files with SATD have more defects compared to files without SATD, (ii) whether SATD changes introduce future defects, and (iii) whether SATD-related changes tend to be more difficult.

software-quality: defects in a file and defect-inducing changes

use the comment patterns to identify SATD

SATD has 2 levels: (1) file-level (2) change-level

Source code as the input, extract the comments. They stor all of the processed data in a PostgreSQL database and query them to answer the research questions.

*** Result
The findings show that there is no clear trend when it comes to defects and self-admitted technical debt.

Their study indicates that although technical debt may have negative effects, its impact is not related to defects, rather making the system more difficult to change in the future.

*** Future work
Further study the nature of the SATD files after they became defective

** Day 5: SE
- *Title*: Identifying self-admitted technical debt through code comment analysis with a contextualized vocabulary
- *Journal*: Information and Software Technology

The following context is referred from [6]

*** Problem
Current detection strategies still return a large number of false positives items when detect SATD using a contextualized vocabulary. Moreover, those strategies do not allow the automatic identification of the type of debt of the identified items.

*** Method
They proposed a contextualized vocabulary model to identify self-admitted technical debt.

They consider decisive patterns as those that bring cues that make it easier to identify a situation of technical debt.

They conduct three empirical studies that: (i) investigated to what extent a pattern is decisive to point to a self-admitted technical debt; (ii) related patterns to TD types; and (iii) investigated the feasibility of using those patterns in practice.

*** Result
The results from the empirical studies show that over half of the ana-
lyzed patterns were considered decisive or very decisive to identify self- admitted technical debt. We also found that composed patterns seem to be more contextualized and decisive than isolated patterns to identify SATD items. Some patterns only make sense when they are combined with other patterns.

*** Future work
We intend to better assess the vocabulary and its accuracy to identify and classify SATD items by comparing the results provided by our approach (vocabulary and eXcomment) with data sourced from manual identification of technical debt by software engineers. We also intend to apply the vocabulary to other repositories, including comments from configuration and version control systems, to investigate how it performs over different types of text comments. Lastly, we want to investi- gate how to combine static source code analysis, software metrics, and code comments analysis to improve the automatic detection of technical debt items.

** Day 6: SE
- *Title*: How Do Companies Collaborate in Open Source Ecosystems? An Empirical Study of OpenStack
- *Proc*: ICSE'20

The following content is referred from [7]
*** Problem
They try to understand how large OSS ecosystems work, and in particular on the patterns of collaboration within one such large ecosystem (i.e. OpenStack)

Previous work has primarily focused on collaboration at the individual level rather than the company level

OpenStack represents a high-potential arena for these companies to play a role in the rapidly evolving cloud computing technology

They adopt a mixed-method research approach (i.e., using both quantitative and qualitative methods)
*** Method
They conducted an empirical study of the OpenStack ecosystem, in which hundreds of companies collaborate on thousands of project repositories to deliver cloud distributions.

They adopted a mixed-method approach that combines an analysis of the version control history with an examination of the peer-reviewed literature and other online documents.

They used OpenStack’s version control data to quantify company collaboration.

*** Result
They find statistically significant evidence that a company’s collaboration position within the network correlates positively with its productivity in terms of the average number of commits its developers make to the OpenStack projects.

*** Future work
- The definitive reason for a strong relationship between companies’ collaboration and productivity remains unclear—developing a better theory that explains this link is an avenue for future work. For example, additional factors for a regression model or conduct qualitative studies at companies that participate in OSS ecosystems.

- Future work could consider other types of contributions, such as participation in online discussions, and reviewing code changes.

- Furthermore, the collaboration between companies could also be explored by studying other interaction channels, such as IRC, mailing list, and issue trackers, in addition to submitting commits to the same projects.

- Future studies could consider a more precise measurement of productivity

** Day 7: SE
- *Title*: Companies' Participation in OSS Development - An Empirical Study of OpenStack
- *Journal*: TSE'19

The following content is referred from [8]
*** Problem
The goal is to investigate how companies contributed developers and commits to OpenStack.
*** Method
They mined the code commit history of OpenStack and analyzed the abundant online records about OpenStack, and its participating companies and individual developers.

This study combined the analysis of code commit history with an examination of the published literature and online documents.

*** Result
volunteer participation is affected by the diversity of companies

*** Future work
How to decide a developer's contribution: future work may be needed to include other activities, e.g., bug fixes, email discussions, and code review changes, to investigate commercial participation in more detail.

** Day 8: SE
- *Title*: Detecting and Quantifying Different Types of Self-Admitted Technical Debt
- *Proc*: 2015 IEEE 7th International Workshop on Managing Technical Debt, MTD 2015

The following content is referred from [9]
*** Problem
Figure out what types of technical debt can be detected using source code comments
*** Method
1. Extract source code comments from 5 well commented open source projects that belongs to different application domains
2. They applied a set of 4 simple filtering heuristics to remove comments that are not likely to contain self-admitted technical debt

Their work is different from the aforementioned work that uses code smells to detect design technical debt since we use code comments to detect technical debt.

*** Result
They found 5 types of self-admitted technical debt which are: design debt, defect debt, documentation debt, requirement debt and test debt

*** Future work
improve the current classification adding more projects to it, increasing the dataset

an advanced technique of natural language processing, which may lead to more automated ways to identify self-admitted technical debt.

** Day 9: SE
- *Title*: What do Programmers Discuss about Deep Learning Frameworks
- *Journal*: EMSE

The following content is referred from [10]
*** Problem
To understand different deep learning frameworks and compare the insights from two platforms, i.e., StackOverflow and GitHub. 
*** Method
latent dirichlet allocation (LDA) topic modeling techniques to derive the discussion topics related to three deep learning frameworks, namely, Tensorflow, PyTorch and Theano
**** Research methodology
1. Data preprocessing
2. Determining Deep Learning workflow
3. Topic Modeling
*** Result
Their observations include 1) a wide range of topics that are discussed about the three deep learning frameworks on both platforms, and the most popular workflow stages are Model Training and Preliminary Preparation. 2) the topic distributions at the workflow level and topic category level on Tensorflow and PyTorch are always similar while the topic distri- bution pattern on Theano is quite different. In addition, the topic trends at the workflow level and topic category level of the three deep learning frameworks are quite different. 3) the topics at the workflow level show different trends across the two platforms. e.g., the trend of the Preliminary Preparation stage topic on Stack Overflow comes to be rela- tively stable after 2016, while the trend of it on GitHub shows a stronger upward trend after 2016.

*** Future work
We tend to analyze how the impact trends of topics at different topic levels
vary with respect to the number of newcomers and the number of unique users and gain some key insights. Moreover, we can also incorporate more deep learning frameworks to make the analysis more common and generalized.


** Day 10: SE
- *Title*: Beyond the Code: Mining Self-Admitted Technical Debt in Issue Tracker Systems
- *Proc*: MSR'20

The following content is referred from [11]
*** Problem
Previous studies mine SATD by searching for specific TD-related terms in source code comments. By contrast, in this paper we argue that developers can admit technical debt by other means, e.g., by creating issues in tracking systems and labelling them as referring to TD.

*** Method
They refer the SATD found in issue tracking systems as issue-based SATD or just SATD-I.

Their intention is to study SATD-I instances that had a practical and positive impact on the projects.

They study SATD-I instances from five open-source systems: GitLab and four GitHub-based systems.

To perform the classification, the tool applies a Naive Bayes Multi-nomial (NBM) technique.

*** Result
Only 29% of the issues that pay TD can be traced to SATD-C. In other words, 71% of the studied issues document and pay TD that would not be possible to identify by considering only source code documentation.

*** Future work
They first intend to enlarge our dataset of SATD-I by mining other tags that may denote TD-related issues. After that, they envision an in-depth analysis of the code transformations performed to pay these debts. Based on this dataset of transformations, they may develop tools and techniques to guide developers on TD payment (e.g., by recommending how to perform changes that contribute to the actual removal of the debt).

* Reference
1. Cotroneo, D., De Simone, L., Liguori, P., Natella, R., & Bidokhti, N. (2019, August). How bad can a bug get? an empirical analysis of software failures in the OpenStack cloud computing platform. In Proceedings of the 2019 27th ACM Joint Meeting on European Software Engineering Conference and Symposium on the Foundations of Software Engineering (pp. 200-211).

2. Zheng, W., Feng, C., Yu, T., Yang, X., & Wu, X. (2019). Towards understanding bugs in an open source cloud management stack: An empirical study of openstack software bugs. Journal of Systems and Software, 151, 210-223.

3. Zhou, P., Liu, J., Liu, X., Yang, Z., & Grundy, J. (2019). Is deep learning better than traditional approaches in tag recommendation for software information sites?. Information and software technology, 109, 1-13.

4. Feng, Z., Guo, D., Tang, D., Duan, N., Feng, X., Gong, M., ... & Zhou, M. (2020). Codebert: A pre-trained model for programming and natural languages. arXiv preprint arXiv:2002.08155.

5. Wehaibi, S., Shihab, E., & Guerrouj, L. (2016, March). Examining the impact of self-admitted technical debt on software quality. In 2016 IEEE 23rd International Conference on Software Analysis, Evolution, and Reengineering (SANER) (Vol. 1, pp. 179-188). IEEE.

6. de Freitas Farias, M. A., de Mendonça Neto, M. G., Kalinowski, M., & Spínola, R. O. (2020). Identifying self-admitted technical debt through code comment analysis with a contextualized vocabulary. Information and Software Technology, 121, 106270.

7. Zhang, Y., Zhou, M., Stol, K. J., Wu, J., & Jin, Z. (2020). How do companies collaborate in open source ecosystems? An empirical study of OpenStack.

8. Zhang, Y., Zhou, M., Mockus, A., & Jin, Z. (2019). Companies' Participation in OSS Development-An Empirical Study of OpenStack. IEEE Transactions on Software Engineering.

9. Maldonado, E. D. S., & Shihab, E. (2015, October). Detecting and quantifying different types of self-admitted technical debt. In 2015 IEEE 7th International Workshop on Managing Technical Debt (MTD) (pp. 9-15). IEEE.

10. Han, J., Shihab, E., Wan, Z., Deng, S., & Xia, X. (2020). What do Programmers Discuss about Deep Learning Frameworks. EMPIRICAL SOFTWARE ENGINEERING.

11. Xavier, L., Ferreira, F., Brito, R., & Valente, M. T. (2020). Beyond the Code: Mining Self-Admitted Technical Debt in Issue Tracker Systems. arXiv preprint arXiv:2003.09418.