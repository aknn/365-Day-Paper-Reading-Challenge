#+TITLE: 365-day-paper-reading-challenge
#+AUTHOR: happygirlzt
#+DATETIME: 2020-06-26 Fri

* To readers
You could fork this repo and start your own 365-day challenge on your branch. Let's study together!

* Schedule
I will read an article related to deep learning, reinforcement learning, natural language processing, and/or software engineering every day. This paper reading is more about a breadth reading. I do not want all my reading is limited to my research project. I hope I could keep pace with the latest research in AI/ML/DL/SE. So, in that case, the paper reading will be in *coarse-grained* manner.

I will try to summarize the key idea of the paper. Currently, I want my daily notes to follow the structure:
1. Problem
2. Method
3. Result
4. Future work

I hope I could keep it work up and improve my ability to read and write.

* Record
** Day 0: SE
- *Title*: How Bad Can a Bug Get? An Empirical Analysis of Software Failures in the OpenStack Cloud Computing Platform
- Tags: bug analysis, fault injection, OpenStack
- Proc.: ESEC/FSE 2019
The following content is referred from [[ref_1][paper 1]]
- Roadmap: Cloud management systems (CMS) are important and usually have residual software bugs. -> Many high-severity failures have been occurring in cloud infrastructures and caused bad results. -> Software developers invest effort in mitigating the consequences of residual bugs. -> They aim to empirically analyze the impact of high-severity failures in the context of a large-scale, industry-applied case study, to pave the way for failure mitigation strategies in CMS. -> OpenStack is widely used and is representative. -> They adopt *software fault injection* to accelerate the occurrence of failures caused by software bugs.
*** Problem
This paper investigated the impact of failures in the context widespread OpenStack CMS. Mainly related to *analyssi of bugs and failures* of cloud systems and *fault injection* in cloud systems.
*** Method
Performing *fault injection* and analyzing the impact of the resulting failures in terms of fail-stop behavior, failure detection through logging, and failure propagation across components.

+ They based fault injection on info on software bugs reported by OpenStack developers and users => characterize frequent bug patterns occurring in the project.

+ They performed a large fault injection campaign on the three subsystems of OpenStack (i.e., Nova, Cinder, and Neutron), a total of 911 experiments.

**** Bug analysis
They went through the problem reports and inspected the related source code. They figured out five most frequent bug types: (1) Wrong parameters value (2) Missing parameters (3) Missing function call (4) Wrong return value (5) Missing exception handlers.
**** Fault injection
They developed a tool to automate this task. The tool uses /ast/ module to generate an /abstract syntax tree (AST)/ representation of the source code. Then, it scans the AST to replace the node with a bug. Finally, it rewrites the modified AST into Python code, using the /astunparse/ Python module.

**** Failure data collection
Use workload generator to record the outcomes of both the API calls and of the assertion checks. They also collect all the log files generated by the CMS.

**** Failure analysis
Two perspectives.
- The first perspective
1. API error
2. Assertion failure
3. Assertion failure(s), followed by an API Error
4. No failure
- The second perspective
1. Failure in the faulty round only
2. Failure in the fault-free round (despite the faulty round)

*** Result
1. In the majority of the experiments, OpenStack failures were not mitigated by a fail-stop behavior. 31.3% of the failures was never notified to the user through exceptions. The others were only notified after a long delay.
2. In 8.5% of the experiments, there was no indication of the failure in the logs. -> high rish for system operators
3. 37.5% of the failures, the injected bugs propagated across several OpenStack components. 68.3% of these failures were notified by a different component from the injected one.
In summary, there is a risk that failures are not timely detected and notified, and they can sliently propagate through the system.
- *Dataset*
1. bug reports from OpenStack: highest severe bugs and have been fixed. ('Critical', 'Fix Committed' or 'Fix Released')
*** Future work
1. Deeper run-time verification of virtual resources
2. Increasing the logging coverage.
Current logging mechanisms in OpenStack only reports high-severity error messages for many of the failures. Some failures with late or no API errors that would benefit from logs to diagnose the failure, but they are missing.
3. Preventing corruptions of persistent data and shared state.

** Day 1: SE
- *Title*: Towards understanding bugs in an open source cloud management stack: An empirical study of OpenStack software bugs
- *Keywords*: Cloud management stack, Bug reports, OpenStack
*** Problem
Conduct an in-depth study to help developers to detect and fix the bugs in CMS.

An empirical study of several key characteristics of bugs in OpenStack.

*** Method
Their study computes general statistics of *50k* OpenStack bugs, including the evolution of bugs, the distribution of bugs, and the duration of bugs. Then, they selected 579 bugs for an in-depth study. They study the input factors for triggering the bugs, the consequences of the bugs, and how the bugs are fixed.

*** Contribution
1. The first comprehensive study on OpenStack bugs.
2. The first large-scale CMS-bug benchmark.
* Reference
1.<<ref_1>>
Cotroneo, D., De Simone, L., Liguori, P., Natella, R., & Bidokhti, N. (2019, August). How bad can a bug get? an empirical analysis of software failures in the OpenStack cloud computing platform. In Proceedings of the 2019 27th ACM Joint Meeting on European Software Engineering Conference and Symposium on the Foundations of Software Engineering (pp. 200-211).
