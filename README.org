#+TITLE: 365-day-paper-reading-challenge
#+AUTHOR: happygirlzt
#+DATETIME: 2020-06-26 Fri

* To readers
You could fork this repo and start your own 365-day challenge on your branch. Let's study together!

* Schedule
I will read an article related to deep learning, reinforcement learning, natural language processing, and/or software engineering every day. This paper reading is more about a breadth reading. I do not want all my reading is limited to my research project. I hope I could keep pace with the latest research in AI/ML/DL/SE. So, in that case, the paper reading will be in *coarse-grained* manner.

I will try to summarize the key idea of the paper. Currently, I want my daily notes to follow the structure:
1. Problem
2. Method
3. Result
4. Future work

I hope I could keep it work up and improve my ability to read and write.
* Contents
1. [[Day 0][day-0-se]]
2. [[Day 1][day-1-se]]
3. [[Day 2][day-2-se]]
4. [[Day 3][day-3-nlp]]

* Record
** Day 0: SE
- *Title*: How Bad Can a Bug Get? An Empirical Analysis of Software Failures in the OpenStack Cloud Computing Platform
- Tags: bug analysis, fault injection, OpenStack
- Proc.: ESEC/FSE 2019
The following content is referred from [1]
- Roadmap: Cloud management systems (CMS) are important and usually have residual software bugs. -> Many high-severity failures have been occurring in cloud infrastructures and caused bad results. -> Software developers invest effort in mitigating the consequences of residual bugs. -> They aim to empirically analyze the impact of high-severity failures in the context of a large-scale, industry-applied case study, to pave the way for failure mitigation strategies in CMS. -> OpenStack is widely used and is representative. -> They adopt *software fault injection* to accelerate the occurrence of failures caused by software bugs.
*** Problem
This paper investigated the impact of failures in the context widespread OpenStack CMS. Mainly related to *analyssi of bugs and failures* of cloud systems and *fault injection* in cloud systems.
*** Method
Performing *fault injection* and analyzing the impact of the resulting failures in terms of fail-stop behavior, failure detection through logging, and failure propagation across components.

+ They based fault injection on info on software bugs reported by OpenStack developers and users => characterize frequent bug patterns occurring in the project.

+ They performed a large fault injection campaign on the three subsystems of OpenStack (i.e., Nova, Cinder, and Neutron), a total of 911 experiments.

**** Bug analysis
They went through the problem reports and inspected the related source code. They figured out five most frequent bug types: (1) Wrong parameters value (2) Missing parameters (3) Missing function call (4) Wrong return value (5) Missing exception handlers.
**** Fault injection
They developed a tool to automate this task. The tool uses /ast/ module to generate an /abstract syntax tree (AST)/ representation of the source code. Then, it scans the AST to replace the node with a bug. Finally, it rewrites the modified AST into Python code, using the /astunparse/ Python module.

**** Failure data collection
Use workload generator to record the outcomes of both the API calls and of the assertion checks. They also collect all the log files generated by the CMS.

**** Failure analysis
Two perspectives.
- The first perspective
1. API error
2. Assertion failure
3. Assertion failure(s), followed by an API Error
4. No failure
- The second perspective
1. Failure in the faulty round only
2. Failure in the fault-free round (despite the faulty round)

*** Result
1. In the majority of the experiments, OpenStack failures were not mitigated by a fail-stop behavior. 31.3% of the failures was never notified to the user through exceptions. The others were only notified after a long delay.
2. In 8.5% of the experiments, there was no indication of the failure in the logs. -> high rish for system operators
3. 37.5% of the failures, the injected bugs propagated across several OpenStack components. 68.3% of these failures were notified by a different component from the injected one.
In summary, there is a risk that failures are not timely detected and notified, and they can sliently propagate through the system.
- *Dataset*
1. bug reports from OpenStack: highest severe bugs and have been fixed. ('Critical', 'Fix Committed' or 'Fix Released')
*** Future work
1. Deeper run-time verification of virtual resources
2. Increasing the logging coverage.
Current logging mechanisms in OpenStack only reports high-severity error messages for many of the failures. Some failures with late or no API errors that would benefit from logs to diagnose the failure, but they are missing.
3. Preventing corruptions of persistent data and shared state.

** Day 1: SE
- *Title*: Towards understanding bugs in an open source cloud management stack: An empirical study of OpenStack software bugs.


The following content is referred from [2]
- *Keywords*: Cloud management stack, Bug reports, OpenStack
*** Problem
Conduct an in-depth study to help developers to detect and fix the bugs in CMS.

An empirical study of several key characteristics of bugs in OpenStack.

*** Method
Their study computes general statistics of *50k* OpenStack bugs, including the evolution of bugs, the distribution of bugs, and the duration of bugs. Then, they selected 579 bugs for an in-depth study. They study the input factors for triggering the bugs, the consequences of the bugs, and how the bugs are fixed.

**** Data collection
They collected bug reports from Launchpad version control system of OpenStack.

There are 6 important levels, e.g., undecided, critical, high, medium, low and wishlist in OpenStack bugs. They focus on Critial, High and Medium bugs that are often more interesting to developers. In addition, they focus on /complete/ and /fixed/ bugs.
**** Basic text parsing
code removal, stop-word removal, stemming and lemmatization
**** Discourse pattern matching
Discourse patterns are rules that caputure the syntax and semantics of the text summarize a family of discourse patterns.

*** Bug triggering factors
From two perspectives: input factors and timing factors
*** Bug consequences
They study the scope of the impact and categorize the symptoms of the failures
*** Bug fixing
They categorize the bug fixes into four types: code fix, configuration fix, environment fix, and test case fix.
*** Contribution
1. The first comprehensive study on OpenStack bugs.
2. The first large-scale CMS-bug benchmark.

*** Result
1. The result indicate a large portion of bugs are related to incorrect configurations.
2. They report the consequences of the manifested bugs and incorrect output is the dominant majority (66.14%).
3. Most of the code fixing involive a small number of lines.

*** Future work
Further research on testing and diagnosis for cloud management stack bugs. It would be promising to investigate techniques that can
direct bug fixing based on the focused code locations, such as using machine learning classifiers to predict error-prone code regions and leveraging fault localization to pinpoint the faults.

** Day 2: SE
- *Title*: Is deep learning better than traditional approaches in tag recommendation for software information sites?

- *Journal*: Information and Software Technology
- *Keywords*: Deep learning, Data analysis, Tag recommendation

The following content is referred from [3]
*** Problem
Whether deep learning is better than traditional approaches in tag recommendation task for software information sites.
**** Formulation
They assume that any software object contains a description and a set of tags. These tags in a software information site and the tags associated with an object is a subset of these tags.

Given a large set of existing software objects that are attached with tags, how to automatically recommend a set of appropriate tags for a new software object.

This is a multi-label classification task.
*** Method
- 4 DL methods: TagCNN, TagRNN, TagHAN (Hierarchical Attention Networks) and TagRCNN (Recurrent Convolutional Neural Networks)
- 3 advanced traditional methods: EnTagRec, TagMulRec, and FastTagRec

A ten-round validation
**** Dataset
One large-scale software information site StackOverflow , 3 medium-scale software information sites Askubuntu, Serverfault, Unix and 6 small-scale sites Codereview, Freecode, Database Administrator, Wordpress, AskDifferent and Software Engineering
**** Evalution metrics
top-k prediction recall, the top-k prediction precision, and the top-k prediction f1-score

*** Result
The performance of TagRNN and TagHAN approaches are worse than traditional approaches in tag recommendation tasks. The performance of TagCNN and TagRCNN approaches are better than traditional approaches in tag recommendation tasks.
*** Future work
How to best represent the software object or artifact with a high quality vector is still a major challenge.

** Day 3: NLP

- *Title*: CodeBERT: A Pre-Trained Model for Programming and Natural Languages

The following context is referred from [4]
*** Problem
CodeBERT is a /bimodal/ pre-trained model for natural language (NL) and programming lan- guage (PL).

modality (n. 形式,形态,特征)

/bimodal/ datapoint is an individual function with paired documentation, and each /unimodal/ code is a function without paired documentation

*** Method
CodeBERT captures the semantic connection between natural language and programming language, and produces general-purpose representations that can broadly support NL-PL understanding tasks (e.g. natural language code search) and generation tasks (e.g. code documentation generation)

CodeBERT is trained on Github code repositories in 6 programming languages.

CodeBERT has exactly the same model architecture as RoBERTa-base.

They regard a piece of code as a sequence of tokens. (WordPiece)
*** Result
CodeBERT achieves state-of-the-art performance on both *natural language code search* and *code documentation generation*

CodeBERT is the first large NL-PL pre-trained model.
*** Future work
A potential direction to improve CodeBERT by incorporating AST.
1. One could learn better generators with bimodal evidence or more complicated neural architecture to improve the replaced token detection objective.
2. The CodeBERT itself could be further improved by generation-related learning objectives.
3. We can apply CodeBERT to more NL-PL related tasks, and extend it to more programming languages

* Reference
1. Cotroneo, D., De Simone, L., Liguori, P., Natella, R., & Bidokhti, N. (2019, August). How bad can a bug get? an empirical analysis of software failures in the OpenStack cloud computing platform. In Proceedings of the 2019 27th ACM Joint Meeting on European Software Engineering Conference and Symposium on the Foundations of Software Engineering (pp. 200-211).

2. Zheng, W., Feng, C., Yu, T., Yang, X., & Wu, X. (2019). Towards understanding bugs in an open source cloud management stack: An empirical study of openstack software bugs. Journal of Systems and Software, 151, 210-223.

3. Zhou, P., Liu, J., Liu, X., Yang, Z., & Grundy, J. (2019). Is deep learning better than traditional approaches in tag recommendation for software information sites?. Information and software technology, 109, 1-13.

4. Feng, Z., Guo, D., Tang, D., Duan, N., Feng, X., Gong, M., ... & Zhou, M. (2020). Codebert: A pre-trained model for programming and natural languages. arXiv preprint arXiv:2002.08155.
