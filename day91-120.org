* Duration
From 24 Sep 2020 to 

* Content
1. [[#day-91-se][Day 91: Towards Accurate Duplicate Bug Retrieval using Deep Learning Techniques]]
2. [[#day-92-se][Day 92: Deeper Text Understanding for IR with Contextual Neural Language Modeling]]

* Day 91: SE
- *Title*: Towards Accurate Duplicate Bug Retrieval using Deep Learning Techniques
- *Year*: 2017
- *Proc*: ICSME

The following content is referred from [1]
** Problem
duplicate bug detection

** Method
They proposed a retrieval and classification model using Siamese Convolution Neural Networks (CNN) and Long Short Term Memory (LSTM) for accurate detection and retrieval of duplicate and similar bugs.

- Unsupervised Approaches: Typical strategy used here is to create a numerical similarity scoring function like cosine similarity.
- 
** Result
They report an accuracy close to 90% and recall rate close to 80%, which makes possible the practical use of such a system.

** Future work
1. Elaborate experimentation is required to train the model effectively, using techniques like batch normalization and various other tweaks. Hyperparameter tuning is done only to a certain extent, more can be done in this regard to improve the results further. The training data is also less for a model of this scale, more training data has to be consolidated for various domains (across projects) to train the model.
2. Incorporating attention mechanism: The currently the model does not have any attention mechanism. Incorporating an appropriate attention mechanism to automatically learn the part of the bug that has to be attended and prioritized, while processing a certain part of duplicate and non duplicate bug, will deﬁnitely yield better results.
3. Other models: Attempting models like Tree-LSTMs for the encodings of single sentence bug descriptions compared to bi-LSTMs. Paragraph vectors and document vectors 5 for the original duplicate/similar bug matching problem should be explored.

* Day 92: NLP
- *Title*: 
- *Year*: 2019
- *Proc*: SIGIR

The following content is referred from [2]
** Problem
leveraging contextual neural language model, BERT, to provide deeper text understanding for IR.

Different from traditional word embeddings, they are contextual – the representation of a word is a function of the entire input text, with word dependencies and sentence structures taken into consideration.

** Method
We adopt a simple passage level approach for document retrieval. We split a document into overlapping passages. The neural ranker predicts the relevance of each passage independently. document score is the score of the first passage (BERT-FirstP), the best passage (BERT-MaxP), or the sum of all passage scores (BERT-SumP). For training, passage-level labels are not available in this work. We consider all passages from a relevant document as relevant and vice versa. When the document title is available, the title is added to the beginning of every passage to provide context.

** Result
Compared to bag-of-words retrieval models, the contextual language model can better leverage language structures, bringing large improvements on queries written in natural languages.

* Reference
1. Deshmukh, J., Podder, S., Sengupta, S., & Dubash, N. (2017, September). Towards accurate duplicate bug retrieval using deep learning techniques. In 2017 IEEE International conference on software maintenance and evolution (ICSME) (pp. 115-124). IEEE.

2. Piwowarski, B., Chevalier, M., Gaussier, E., Maarek, Y., Nie, J.-Y., Scholer, F., Dai, Z., & Callan, J. (2019). Deeper Text Understanding for IR with Contextual Neural Language Modeling. ArXiv, 985–988. https://doi.org/10.1145/3331184.3331303